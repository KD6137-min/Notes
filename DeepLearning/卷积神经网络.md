# 卷积神经网络

## 前馈神经网络

Feedforward Neural Network，FNN，一种人工神经网络，其中信息在网络中单向传播，从输入层经过一个或多个隐藏层，最终到达输出层

特点：没有反馈连接，数据只能沿着一个方向流动，从输入到输出，所有的节点之间通过加权连接相连

分类：

1. **全连接神经网络（Fully Connected Network, FCN）** ：

    - 每一层的每个神经元都与下一层的每个神经元相连接
    - 常用于处理结构化数据，如表格数据
2. **卷积神经网络（Convolutional Neural Network, CNN）** ：

    - 主要用于图像处理，通过局部连接和权重共享来提取图像特征
    - 包含卷积层、池化层等结构，具有较强的空间特征提取能力
3. **递归神经网络（Recurrent Neural Network, RNN）** ：

    - 尽管严格来说，RNN有反馈环路，但其基本单元的前馈部分也是前馈神经网络的一种形式。用于处理序列数据（如时间序列、自然语言处理等）
    - 变种包括长短期记忆网络（LSTM）和门控循环单元（GRU）
4. **自编码器（Autoencoder）** ：

    - 一种无监督学习的网络，通过压缩输入数据到隐藏层再重构输入
    - 用于数据降维和特征学习
5. **生成对抗网络（Generative Adversarial Network, GAN）** ：

    - 包含两个部分：生成器和判别器，虽然其结构较为复杂，但生成器部分可以看作是一个前馈网络
6. **多层感知器（Multilayer Perceptron, MLP）** ：

    - 是最基本的前馈神经网络，由多个层构成，通常用于分类和回归问题，激活函数如ReLU、Sigmoid等

‍

## 反馈神经网络

Feedback Neural Network，具有循环连接结构的神经网络，能够通过内部反馈机制处理时序数据和动态系统

与前馈神经网络（FNN）的关键区别在于**信息流动方向**：

- **前馈网络**：数据单向流动（输入→隐藏层→输出），无循环连接，适合静态数据（如图像分类）
- **反馈网络**：允许数据循环流动（如输出→隐藏层或层内自连接），形成动态记忆，适合时序任务（如语音识别、股票预测）

**核心特点：**

- **循环结构**：通过隐藏状态$h_t$保存历史信息，数学表示为：  
  $h_t = \sigma(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$  
  其中$\sigma$为激活函数（如tanh），$W_{hh}$和$W_{xh}$为权重矩阵
- **动态处理**：每个时间步的输出依赖当前输入和前一状态，适合序列建模

**主要类型：**

- **循环神经网络（RNN）** ：基础反馈结构，但存在梯度消失问题
- **长短期记忆网络（LSTM）** ：引入遗忘门、输入门和输出门，解决长期依赖问题
- **Hopfield网络**：全连接结构，用于联想记忆和优化问题
- 玻尔兹曼机（Boltzmann Machine）：基于概率的生成模型

‍

## FFN层

Feed-Forward Network，常出现在编码器和解码器的注意力层之后，通过非线性变换增强模型的表达能力，帮助捕捉更复杂的特征

结构：常由两个全连接层（线性变换）和一个激活函数组成

作用：

- **增强非线性**：弥补自注意力机制的线性局限性，学习更复杂的特征
- **位置独立处理**：对每个位置的输入独立操作，与注意力机制的全局交互形成互补
- **知识存储**：部分研究认为FFN层以键值对（KV）形式存储训练数据的知识

‍

## 池化

Pooling，对特征图进行降维和压缩，“集中”“汇聚”“汇总”，把很多信息集中到一起

|||
| -------------| ------------------------------|
|Pool|一个区域的数据块|
|Pooling|将多个数值合并成一个代表性值|
|Max Pooling|每个小区域中选最大的数值|
|Avg Pooling|每个小区域中取平均值|

**卷积层的卷积核（filters）本质上就相当于全连接层中要学习的权重参数**，作用于输入的**局部区域**，通过**权重共享**降低参数量，并提取局部特征

‍

## 块和层

块：block，多个**层**组成的更大结构，通常完成一组相关的功能，如一个卷积块包含卷积层、激活层、归一化层等，常见卷积块、残差块、编码器块、解码器块

层：layer，神经网络中的基本构建单元，常见全连接层、卷积层、池化层、激活层、归一化层

# LeNet

![截屏2025-05-16 22.09.04](../assets//截屏2025-05-16%2022.09.04-20250604223302-13luh9x.png)

1. 输入：单通道，32\*32
2. 第一卷积核：6个，5\*5
3. AvgPool：无重叠，2\*2
4. 第二卷积核：16个，5\*5
5. AvgPool：无重叠，2\*2
6. 全连接层转换到类别空间

```python
 class Reshape(torch.nn.Module):
     def forward(self, x):
         return x.view(-1, 1, 28, 28)
     
 net = torch.nn.Sequential(
     Reshape(), nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
     nn.AvgPool2d(2, stride=2),
     nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
     nn.AvgPool2d(2, stride=2), nn.Flatten(),
     nn.Linear(16*5*5, 120), nn.Sigmoid(),
     nn.Linear(120, 84), nn.Sigmoid(),
     nn.Linear(84, 10)
 )
```

# ImageNet

非模型，大规模图像数据库，李飞飞发起，手动标注类别

> ILSVRC: ImageNet Large Scale Visual Recognition Chanllenge，年度比赛

# AlexNet

更深更大的LeNet(10x参数个数，260x计算复杂度)，主要改进：

- 隐藏全连接层后加入了dropout层
- 做了数据增强
- 激活函数从Sigmoid换成ReLU，减缓梯度消失
- MaxPooling
- 引导计算机视觉方法论的改变，不再人工做特征提取，而是用CNN学习特征

架构：

|层名|类型|卷积核数量（通道数）|卷积核大小|步幅 / 池化|改进|输出尺寸 (ImageNet 输入为 224×224×3)|
| --------------| ----------| ----------------------| ------------| --------------| --------------------------------| ----------------------------------------|
|Conv1|卷积层|96|11×11|stride\=4|更大的核窗口和步长|55×55×96|
|MaxPool1|最大池化|-|3×3|stride\=2|更大的池化窗口，使用最大池化层|27×27×96|
|Conv2|卷积层|256|5×5|stride\=1|更多的输出通道|27×27×256|
|MaxPool2|最大池化|-|3×3|stride\=2||13×13×256|
|Conv3|卷积层|384|3×3|stride\=1|新增3层卷积层|13×13×384|
|Conv4|卷积层|384|3×3|stride\=1||13×13×384|
|Conv5|卷积层|256|3×3|stride\=1||13×13×256|
|MaxPool3|最大池化|-|3×3|stride\=2||6×6×256|
|FC1|全连接层|4096|-|-|从120增加到4096|-|
|FC2|全连接层|4096|-|-||-|
|FC3 (Output)|全连接层|1000（分类数）|-|-||-|

```python
  net = nn.Sequential(
     nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
     nn.MaxPool2d(kernel_size=3, stride=2),
     nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
     nn.MaxPool2d(kernel_size=3, stride=2),
     nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), 
     nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), 
     nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), 
     nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
     nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5),
     nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),
     nn.Linear(4096, 1000)
 )
```

# VGG

使用可重复的**卷积块**构建深度卷积神经网络，多个VGG块后接全连接层，不同次数的重复块得到不同的架构

更大更深更规则的AlexNet

VGG块：

- 3\*3卷积(pad 1)，n层，m通道
- 2\*2MaxPool，stride 2

VGG架构：

![截屏2025-05-19 08.53.50](../assets//截屏2025-05-19%2008.53.50-20250604223356-vrtd94d.png)​

```python
 def vgg_block(num_convs, in_channels, out_channels):
     layers = []
     for _ in range(num_convs):
         layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
         layers.append(nn.ReLU())
         in_channels = out_channels
     layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
     return nn.Sequential(*layers)
 
 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
 
 def vgg(conv_arch):
     conv_blks = []
     in_channels = 1
     for (num_convs, out_channels) in conv_arch:
         conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
         in_channels = out_channels
         
     return nn.Sequential(
         *conv_blks, nn.Flatten(),
         nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(),
         nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(),
         nn.Dropout(0.5), nn.Linear(4096, 10)
     )
     
 net = vgg(conv_arch)
```

# NiN

Network in Network，网络中的网络，引入小型`网络模块`​来提升表达能力（模块化思想）

传统CNN的问题：缺乏对特征间非线性组合的表达能力

核心思想：用MLP替代传统卷积层的线性卷积核

NiN块(MLPConv层)：

- Convolution
- 1\*1 Convolution，stride 1，pad 0，输入形状同卷积层输出，起到全连接层的作用
- 1\*1 Convolution，stride 1，pad 0，输入形状同卷积层输出，起到全连接层的作用

1x1 卷积：不改变空间尺寸，但能实现 **通道之间的非线性组合**，相当于对每个位置进行一个小型全连接网络（MLP），对每个像素增加了非线性性：

|功能|描述|
| ------------| ------------------------------------|
|特征混合|把不同通道的信息进行融合|
|增加非线性|叠加非线性激活（ReLU）提升表达能力|
|降低参数量|比大卷积核参数少，计算快|

NiN 结构（简化版，输入图像为 224×224）：

|层|类型|输出通道数|卷积核大小|步幅|
| ----| ----------------| ------------| ------------| ------|
|1|Conv + 1x1×2|96|11×11|4|
|2|MaxPool||3×3|2|
|3|Conv + 1x1×2|256|5×5|1|
|4|MaxPool||3×3|2|
|5|Conv + 1x1×2|384|3×3|1|
|6|MaxPool||3×3|2|
|7|Conv + 1x1×2|10（类别）|3×3|1|
|8|Global AvgPool|-|-|-|

- 交替使用NiN块和MaxPool，逐步减小高宽和增大通道数
- 最后一层使用 **全局平均池化（Global Average Pooling）**  代替全连接层，其输入通道数为类别数，不容易过拟合，更少的参数个数

```python
 def nin_block(in_channels, out_channels, kernel_size, strides, padding):
     return nn.Sequential(
         nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(),
         nn.Conv2d(out_channels, out_channels, kernel_size), nn.ReLU(),
         nn.Conv2d(out_channels, out_channels, kernel_size), nn.ReLU()
     )
     
 net = nn.Sequential(
     nin_block(1, 96, kernel_size=11, strides=4, padding=0),
     nn.MaxPool2d(3, stride=2),
     nin_block(96, 256, kernel_size=5, strides=1, padding=2),
     nn.MaxPool2d(3, stride=2),
     nin_block(256, 384, kernel_size=3, strides=1, padding=1),
     nn.MaxPool2d(3, stride=2),
     nn.Dropout(0.5), 
     nin_block(384, 10, kernel_size=3, strides=1, padding=1),
     nn.AdaptiveAvgPool2d((1, 1)),
     nn.Flatten()
 )
```

# GoogLeNet

含并行连接的网络

![截屏2025-05-19 19.26.28](../assets//截屏2025-05-19%2019.26.28-20250604223620-a3opkak.png)​

Inception块：4个路径从不同层面抽取信息，在输出通道维合并，模型参数小，计算复杂度低

![截屏2025-05-19 19.28.33](../assets//截屏2025-05-19%2019.28.33-20250604223637-t6147e5.png)

- 白色的卷积用来改变通道数，蓝色的卷积用来抽取信息
- 最左边的1\*1卷积用来抽取通道信息，其他3\*3卷积用来抽取空间信息
- 与单3\*3或5\*5卷积层比，Inception有更少的参数个数和计算复杂度

5段，9个inception块：

![截屏2025-05-19 21.19.47](../assets//截屏2025-05-19%2021.19.47-20250604223654-ul6x7m6.png)

- 段1&2：更小的宽口，更多的通道

  ![截屏2025-05-19 21.21.09](../assets//截屏2025-05-19%2021.21.09-20250604223712-vwxmtlx.png)
- 段3：

  ![截屏2025-05-19 21.23.42](../assets//截屏2025-05-19%2021.23.42-20250604223723-p7yxewm.png)
- 段4&5：

  ![截屏2025-05-19 21.24.05](../assets//截屏2025-05-19%2021.24.05-20250604223747-u84clgj.png)

后续变种：

- Inception-BN(v2)：使用batch normalization
- Inception-V3：修改了Inception块：

  - 5\*5替换为多个`3*3卷积层`​
  - 5\*5替换为`1*7和7*1卷积层`​
  - 3\*3替换为`1*3和3*1卷积层`​

  ![截屏2025-05-19 21.30.43](../assets//截屏2025-05-19%2021.30.43-20250604223817-ega81jt.png)

  ![截屏2025-05-19 21.31.21](../assets//截屏2025-05-19%2021.31.21-20250604223826-0zbjt3d.png)
- Inception-V4：使用残差连接

```python
 class  Inception(nn.Module):
     def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
         super(Inception, self).__init__(**kwargs)
         self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
         self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
         self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
         self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
         self.p3_2 = nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)
         self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
         self.p4_2 = nn.Conv2d(in_channels,c4,kernel_size=1)
         
     def forward(self, x):
         p1 = F.relu(self.p1_1(x))  # 第一条路的输出
         p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) # 第二条路的输出
         p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
         p4 = F.relu(self.p4_2(self.p4_1(x)))
         return torch.cat((p1, p2, p3, p4), dim=1) # 批量大小的dim为0，通道数的dim为1，以通道数维度进行合并
 
 b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),
                   nn.ReLU(),nn.MaxPool2d(kernel_size=3,stride=2,padding=1))  
 
 b2 = nn.Sequential(nn.Conv2d(64,64,kernel_size=1),nn.ReLU(),
                   nn.Conv2d(64,192,kernel_size=3,padding=1),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
 
 b3 = nn.Sequential(Inception(192,64,(96,128),(18,32),32),
                   Inception(256,128,(128,192),(32,96),64),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
 
 b4 = nn.Sequential(Inception(480,192,(96,208),(16,48),64),
                   Inception(512,160,(112,224),(24,64),64),
                   Inception(512,128,(128,256),(24,64),64),
                   Inception(512,112,(144,288),(32,64),64),
                   Inception(528,256,(160,320),(32,128),128),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
 
 b5 = nn.Sequential(Inception(832,256,(160,320),(32,128),128),
                    Inception(832,384,(192,384),(48,128),128),
                   nn.AdaptiveAvgPool2d((1,1)),nn.Flatten())
 
 net = nn.Sequential(b1,b2,b3,b4,b5,nn.Linear(1024,10))
```

**​`nn.AdaptiveAvgPool2d`​**​对输入应用自适应平均池化，将feature map改为需要大小的输出，只需给定输出特征图的大小，其中通道数前后不发生变化

# ResNet

残差网络，在堆叠层数的同时不会增加模型复杂度

核心思想：每个附加层都应该更容易地包含原始函数作为其元素之一，每一次增加函数复杂度之后函数所覆盖的区域会包含原来函数所在的区域（nested function），只有当复杂包含简单函数类时才能确保提高性能

ResNet如何避免梯度消失：将乘法运算变成加法运算（残差连接）

残差块：输入可通过跨层数据线路更快地向前传播

![截屏2025-05-20 17.48.50](../assets//截屏2025-05-20%2017.48.50-20250604223905-t190wcu.png)

两种实现：沿用VGG的3\*3卷积层设计

![截屏2025-05-20 17.52.32](../assets//截屏2025-05-20%2017.52.32-20250604223913-5p5skth.png)

- 第一种实现（左图）：直接将输入加在了叠加层的输出上，要求两个卷积层的输出与输入形状相同
- 第二种实现（右图）：先对输入进行1\*1卷积变换通道（改变范围），再加入到叠加层的输出上，可改变通道数

不同的残差块：可以加在不同的位置

![截屏2025-05-20 18.04.47](../assets//截屏2025-05-20%2018.04.47-20250604224012-oopipw8.png)

两种不同的ResNet块：不同数量的搭配可组成不同的网络

- 高宽减半：stride 2，一般用于第一个
- 高宽不变：一般重复多次，接在高宽减半的块之后

ResNet架构：类似VGG和GoogLeNet的总体架构

![截屏2025-05-20 18.18.37](../assets//截屏2025-05-20%2018.18.37-20250604224017-23ejfp1.png)

```python
 class Residual(nn.Module):
     def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
         super().__init__()
         self.conv1 = nn.Conv2d(
             input_channels, num_channels, kernel_size=3, padding=1, stride=strides
         )
         self.conv2 = nn.Conv2d(
             num_channels, num_channels, kernel_size=3, padding=1
         )
         if use_1x1conv:
             self.conv3 = nn.Conv2d(
                 input_channels, num_channels, kernel_size=1, stride=strides
             )
         else:
             self.conv = None
         self.bn1 = nn.BatchNorm2d(num_channels)
         self.bn2 = nn.BatchNorm2d(num_channels)
         self.relu = nn.ReLU(inplace=True)
     
     def forward(self, X):
         Y = F.relu(self.bn1(self.conv1(X)))
         Y = self.bn2(self.conv2(Y))
         if self.conv3:
             X = self.conv3(X)
         Y += X
         return F.relu(Y)
 
 # 输入与输出形状一致
 blk = Residual(3, 3)
 X = torch.rand(4, 3, 6, 6)
 Y = blk(X)
 
 # 增加输出通道同时减半输出的高宽
 blk = Residual(3, 6, use_1x1conv=True, strides=2)
```

## 残差连接

Residual Connection，把输入直接“跳跃”连接到后面的层输出，解决深层网络训练时梯度消失/爆炸的问题，训练更快、更稳定，性能更好

$$
Output = F(x) + x
$$

- x：输入
- F(x)：一系列操作（如线性变换 + 激活 + Dropout等）
- 最终输出是 F(x) 和 x 的**加法**

原理：允许模型学“偏差”而不是“全部”

‍

## 空洞卷积

Dilated Convolution，又称Atrous Convolution，在卷积核内部引入间隔，让每个卷积操作跳过一定的输入位置，用于扩大感受野而不增加参数数量或降低分辨率

普通卷积中，感受野增长缓慢，常需叠加多层卷积或使用池化（会降低分辨率）

卷积核大小为 k，空洞率（dilation rate）为 r，则覆盖的感受野为：

$$
k_{eff} = k + (k-1)(r-1)
$$

参数dilation即可控制

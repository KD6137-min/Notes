# 特殊函数

## 一. 激活函数

给神经元的输出增加`非线性`​，使模型能拟合复杂的函数

数学形式：ϕ 就是激活函数，$y = \phi(w^T x + b)$

### 常见的激活函数：

- `sigmoid(x)`​
- `tanh(x)`​
- `ReLU(x)`​：最常用，对负数整流，计算快，可产生稀疏的激活，提高泛化能力，$ReLU(x) = max(0, x)$
- `LeakyReLU(x)`​：改进ReLU，允许负值有一个小的斜率，避免输出完全为零，$LeakyReLU(x) = max(\alpha x, x)$
- `Softmax`​：一般在最后一层，非负，归一化，概率化，放大差异，满足$p_i \ge0$，以及$\sum^n_{i=1}p_i=1$  

  $$
  p_i = \frac{exp(z_i)}{\sum^{n}_{j=1}exp(z_j)}
  $$

## 二. 指示函数

又称**特征函数**，是一个在给定集合或条件下，用于指示元素是否属于该集合的函数

定义：设 A 是一个集合，则1\_{A}(x)为 **集合 A 的指示函数**:

$$
1_A(x) = \begin{cases}
1, & \text{如果 } x \in \{2,4,6\} \\
0, & \text{如果 } x \notin \{2,4,6\}
\end{cases}
$$

## 三. 核函数

用来在不显式计算高维映射的前提下，**计算高维空间中两个向量的内积**，本质是“用一个函数 K 计算两个样本在隐空间的相似度”

#### 高斯核

Gaussian Kernel，也叫**径向基函数核（RBF核，Radial Basis Function Kernel）** ，常见的**相似度度量函数**

可以捕捉更复杂的、非线性关系，非常适合数据不能线性分割的场景

接受两个向量 x 和 x′，输出一个数，表示它们之间的“相似程度”：

$$
K(x,x') = exp(- \frac{\|x-x'\|^2}{2 \sigma^2})
$$

或者:

$$
K(x,x')=exp(-\gamma\|x-x'\|^2)
$$

其中：

- ||x−x||^2：欧几里得距离的平方

- σ 控制函数的“宽度”，$\gamma=\frac{1}{2\sigma^2}$ 控制核函数的“紧密程度”

可看成：一个“以 x 为中心的高斯钟形曲线”，再用它来评估另一个点 x′ 落在钟形曲线上的“高度”，x′ 离 x 越近，相似度 K(x,x′) 越接近 1；x′ 离 x 越远，相似度越接近 0

## 四. 推理函数

从已知信息推导出新结论的数学或逻辑工具，将前提映射到结论的函数，通常满足逻辑一致性

eg：逻辑蕴含（若 A $\rightarrow$ B，则 A 为真时 B 必为真）、贝叶斯定理

## 五. RealSoftMax

$$
\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))
$$

**与最大值的关系**：输出总是大于输入中的最大值

$$
\mathrm{RealSoftMax}(a, b) > \max(a, b)
$$

**尺度缩放性质**： 对于标量 λ\>0，有：

$$
\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) > \max(a, b)
$$

且当 λ→∞ 时，该表达式收敛于 max⁡(a,b)，因此可视为最大值函数的平滑近似

**扩展性**： 可推广到多个输入值：

$$
\mathrm{RealSoftMax}(x_1, x_2, \dots, x_n) = \log \left( \sum_{i=1}^n \exp(x_i) \right)
$$

**与Softmax的区别**：

- **用途不同**：RealSoftMax是数学上的严格定义，softmax通常指多分类概率归一化函数
- **输出形式**：RealSoftMax输出一个标量，而softmax输出概率分布向量

## 六. Softmax交叉熵损失数值稳定性技巧

避免直接计算Softmax时可能出现的数值溢出问题（尤其是当输入值较大时）

**原始公式问题：**

$$
\hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$

**问题**：如果某个 $o_k$ 很大（如1000），$\exp(o_k)$ 会超出浮点数表示范围（数值溢出），导致计算失败

**技巧**：**对输入进行平移**，利用数学性质简化计算：

$$
\hat{y}_j = \frac{\exp(o_j - C)}{\sum_k \exp(o_k - C)}
$$

其中 $C = \max(o_k)$（所有输出中的最大值），减去最大值后，最大的 $\exp(o_k - C)$ 变为 $\exp(0) = 1$，其他值在0到1之间，避免了数值溢出

**对数似然损失（Cross-Entropy）的进一步优化**：交叉熵损失需要计算 $\log(\hat{y}_j)$，而通过上述平移后：

$$
\log(\hat{y}_j) = (o_j - C) - \log\left(\sum_k \exp(o_k - C)\right)
$$

这样直接利用 $o_j - C$ 计算，避免了先算Softmax再取对数的数值不稳定问题

Softmax的数学定义未变，只是通过数学等价变形（平移输入）优化计算过程

PyTorch的`CrossEntropyLoss`​内部通过以下两步优化，跳过了显式计算Softmax的步骤：

1. **数学变形**：利用对数Softmax的展开式，将计算合并为一步
2. **数值稳定技巧**：通过平移输入（减去最大值）和`LogSumExp`​，避免直接计算大数的指数

> 总结：计算Softmax时内部会减最大值优化，计算交叉熵损失时内部通过优化绕过Softmax

## 七. Huber损失函数

结合了均方误差（MSE）和平均绝对误差（MAE）优点，常用于处理包含异常值的数据

核心思想：通过一个阈值参数 $\delta$ 动态调整损失的计算方式：

$$
L_\delta(a) = \begin{cases} 
\frac{1}{2}a^2 & \text{若 } |a| \leq \delta, \\
\delta(|a| - \frac{1}{2}\delta) & \text{若 } |a| > \delta 
\end{cases}
$$

其中 $a = y - \hat{y}$ 为真实值与预测值的误差，$\delta$ 是用户定义的阈值

**特点：**

- **鲁棒性**：

  - 当误差较小时（$|a| \leq \delta$），采用平方损失（类似MSE），保证梯度平滑
  - 当误差较大时（$|a| > \delta$），转为线性损失（类似MAE），减少异常值的影响
- **可微性**：Huber损失在 $\delta$ 处连续且可导，优化时比MAE更稳定
- **灵活性**：通过调整 $\delta$ 可平衡MSE和MAE的特性，例如 $\delta \to 0$ 时接近MAE，$\delta \to \infty$ 时接近MSE

**代码实现**

```python
import torch.nn as nn
loss_fn = nn.SmoothL1Loss(beta=1.0)  # beta即δ参数
```

**对比其他损失函数**：

- **MSE**：对异常值敏感，但收敛快
- **MAE**：对异常值鲁棒，但梯度恒定可能导致收敛慢
- **Huber损失**：折中两者，适合需要兼顾鲁棒性和训练效率的场景

# 微积分

## 梯度

||x|$\mathbf{x}$​|
| ----| ----| ----|
|y|$\dfrac{\partial y}{\partial x}$​|$\dfrac{\partial y}{\partial \mathbf{x}}$​|
|$\mathbf{y}$​|$\dfrac{\partial \mathbf{y}}{\partial x}$​|$\dfrac{\partial \mathbf{y}}{\partial \mathbf{x}}$​|

|表达式|$\dfrac{\partial y}{\partial \mathbf{x}}$表达式|
| --------| --------|
|$y = a$​|$\mathbf{0}^T$​|
|$y = au$​|$a \dfrac{\partial u}{\partial \mathbf{x}}$​|
|$y = \mathrm{sum}(\mathbf{x})$​|$\mathbf{1}^T$​|
|$y = \\|\mathbf{x}\\|^2$​|$2\mathbf{x}^T$​|
|$y = u + v$​|$\dfrac{\partial u}{\partial \mathbf{x}} + \dfrac{\partial v}{\partial \mathbf{x}}$​|
|$y = uv$​|$\dfrac{\partial u}{\partial \mathbf{x}} v + \dfrac{\partial v}{\partial \mathbf{x}} u$​|
|$y = \langle \mathbf{u}, \mathbf{v} \rangle$​|$\mathbf{u}^T \dfrac{\partial \mathbf{v}}{\partial \mathbf{x}} + \mathbf{v}^T \dfrac{\partial \mathbf{u}}{\partial \mathbf{x}}$​|
|$y = \mathbf{a}$​|$\mathbf{0}$​|
|$y = \mathbf{x}$​|$\mathbf{I}$​|
|$y = A\mathbf{x}$​|$A$​|
|$y = \mathbf{x}^T A$​|$A^T$​|
|$y = a\mathbf{u}$​|$a \dfrac{\partial \mathbf{u}}{\partial \mathbf{x}}$​|
|$y = A\mathbf{u}$​|$A \dfrac{\partial \mathbf{u}}{\partial \mathbf{x}}$​|
|$y = \mathbf{u} + \mathbf{v}$​|$\dfrac{\partial \mathbf{u}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{v}}{\partial \mathbf{x}}$​|

a is not a function of $\mathbf{x}$， **0** and **1** are vectors

$$
\begin{align}
\mathbf{y} &= 
\begin{bmatrix}
    y_1 \\ 
    y_2 \\
    \vdots \\ 
    y_m
\end{bmatrix}
&
\frac{\partial \mathbf{y}}{\partial x} &= 
\begin{bmatrix}
    \frac{\partial y_1}{\partial x} \\ 
    \frac{\partial y_2}{\partial x} \\
    \vdots \\ 
    \frac{\partial y_m}{\partial x}
\end{bmatrix}
\end{align}
$$

$\partial y /\partial \mathbf{x}$是行向量（分母布局符号）

$\partial \mathbf{y} / \partial x$是列向量（分子布局符号）

$$
\mathbf{x} = 
\begin{bmatrix}
    x_1 \\ 
    x_2 \\
    \vdots \\ 
    x_n
\end{bmatrix},
\quad
\mathbf{y} = 
\begin{bmatrix}
    y_1 \\ 
    y_2 \\
    \vdots \\ 
    y_m
\end{bmatrix}
$$

$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = 
\begin{bmatrix}
	\frac{\partial y_1}{\partial \mathbf{x}} \\
	\frac{\partial y_2}{\partial \mathbf{x}} \\
	\vdots \\
	\frac{\partial y_m}{\partial \mathbf{x}} \\
\end{bmatrix}
=
\begin{bmatrix}
    \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\
    \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \cdots & \dfrac{\partial y_2}{\partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \dfrac{\partial y_m}{\partial x_1} & \dfrac{\partial y_m}{\partial x_2} & \cdots & \dfrac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$

## 计算图的反向传播

导数的传递：

![截屏2025-04-25 20.24.31](../assets/截屏2025-04-25%2020.24.31-20250605123435-29j444w.png)

- 加法节点：原封不动流向下一个节点

  ![截屏2025-04-25 20.27.57](../assets/截屏2025-04-25%2020.27.57-20250605123447-srbd58t.png)
- 乘法节点：乘输入信号的翻转值  
  ​![截屏2025-04-25 20.29.10](../assets/截屏2025-04-25%2020.29.10-20250605123504-fphcnt4.png)
- ReLU节点：  
  ​![截屏2025-04-25 20.31.01](../assets/截屏2025-04-25%2020.31.01-20250605123510-4yz03qu.png)
- `/`​节点：即$y = \frac{1}{x}$，导数有解析解：$𝜕𝑦/𝜕𝑥=−1/𝑥^2=−𝑦^2$，即直接乘上游平方的负数
- Sigmoid节点：$\frac{\partial L}{\partial y}y^2exp(-x)$，可整理为$\frac{\partial L}{\partial y}y(1-y)$  
  ​![截屏2025-04-25 21.22.52](../assets/截屏2025-04-25%2021.22.52-20250605123525-xu2e87c.png)

  ![截屏2025-04-25 21.25.59](../assets/截屏2025-04-25%2021.25.59-20250605123607-i502ri3.png)
- 正向传播有分支时，反向传播先求和再算节点导数

  ![截屏2025-04-25 21.28.14](../assets/截屏2025-04-25%2021.28.14-20250605123623-sj6nffk.png)

# 线性代数

## 仿射变换

Affine Transformation，常见的图像变换方法，包括旋转、平移、缩放、剪切

可以保持直线、平行线、比例关系等特性，但不保持角度和长度

定义：是一种线性变换加上平移变换，可通过一个 **2x3 的矩阵** 来表示，变换后的点通过以下公式计算：

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} & t_x \\ a_{21} & a_{22} & t_y \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
$$

- (x,y) 是原始点的坐标，(x′,y′) 是变换后点的坐标
- a11,a12,a21,a22 是仿射变换的线性变换部分（例如旋转、缩放、剪切），tx,ty 是平移部分

### 四种基本操作

- 平移 (Translation)：沿向量 (t\_x, t\_y) 移动像素位置

  $$
  \begin{cases}
  x' = x + t_x \\
  y' = y + t_y
  \end{cases}
  $$
- 缩放 (Scaling)：改变坐标轴比例尺度

  $$
  \begin{cases}
  x' = s_x \cdot x \\
  y' = s_y \cdot y
  \end{cases}
  $$

  - s\_x, s\_y 分别为x/y方向缩放因子，等比缩放时 s\_x \= s\_y
- 旋转 (Rotation)：绕原点逆时针旋转θ角

  $$
  \begin{cases}
  x' = x\cos\theta - y\sin\theta \\
  y' = x\sin\theta + y\cos\theta
  \end{cases}
  $$

  - 默认绕坐标原点旋转，实际应用常需先平移到中心点
- 剪切 (Shearing)：沿特定方向倾斜像素

  $$
  \begin{cases}
  x' = x + \alpha \cdot y \\
  y' = y + \beta \cdot x
  \end{cases}
  $$

  - α控制水平剪切强度，β控制垂直剪切强度

## 范数

衡量向量或矩阵“大小”的工具，不同种类的范数适用于不同的场景，满足非负性、齐次性、三角不等式（严格范数需额外满足次可乘性）

- **Lp 范数（向量范数、p-范数）** ：对向量$\mathbf{x} = (x_1, x_2, \dots, \_n)$，其 Lp 范数为：  $\|\mathbf{x}\|_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}$  

  - $L^1$ 范数（曼哈顿范数）：$\| \mathbf{x} \| _1 = \sum |x_i|$，用于稀疏性优化
  - $L^2$ 范数（欧几里得范数）：$\|\mathbf{x}\|_2 = \sqrt{\sum x_i^2}$，最常用的几何距离
  - $L^\infty$ 范数（切比雪夫范数）：$\|\mathbf{x}\|_\infty = \max |x_i|$，关注最大误差
  - $L^0$范数：非零元素个数（常用于稀疏性，但非严格范数）
  - $L^{p,q}$ 混合范数：结合不同维度的 Lp 和 Lq 范数
- **Frobenius 范数（矩阵范数）** ：对矩阵$A \in \mathbb{R}^{m \times n}$ ，其 Frobenius 范数为：$\|A\|_F= \sqrt{\sum_{i,j} A_{ij}^2}$  

  - 是向量 L\^2 范数在矩阵上的推广
- **迹范数（Nuclear Norm）** ：矩阵奇异值之和，即$\|A\|_* = \sum \sigma_i$，其中  $\sigma_i$是奇异值
- **诱导范数（算子范数）** ：矩阵范数的一种，由向量范数导出，定义为：$\|A\| = \sup_{\mathbf{x} \ne 0} \frac{\| \mathbf{Ax} \|}{\| \mathbf{x} \|}$，或$\|A\| = max_{\|x\|=1}\|Ax\|$  

  - $L^1$ 诱导范数：$\|A\|_1 = \max_j \sum_i |A_{ij}|$（列和最大值）
  - **谱范数（**  **$L^2$**​**诱导范数）** ：$\|A\|_2 = \sigma_{\text{max}}(A)$，即最大奇异值
  - $L^\infty$ 诱导范数：$\|A\|_\infty = \max_i \sum_j |A_{ij}|$（行和最大值）
- **Schatten范数**：广义化迹范数，定义为奇异值的 $L^p$ 范数：$\|A\|_{S_p} = \left( \sum \sigma_i^p \right)^{1/p}$  

  - 当 p\=1 时为迹范数，p\=2 时为 Frobenius 范数
- **核范数（Kernel Norm）** ：核矩阵的迹范数，用于核方法中的低秩表示
- **对偶范数**  ：对偶空间中的范数，例如 $L^p$ 的对偶是$L^q$（满足 1/p + 1/q \= 1）
- **条件数（Condition Number）**   ：矩阵的最大奇异值与最小奇异值之比，衡量数值稳定性
- **自定义加权范数**  ：如$\|A\|_{w} = \sqrt{\sum w_i A_{ij}^2}$ ，用于强调特定维度的重要性

## **奇异值**和**特征值**

- **特征值（Eigenvalue）** ：一个**方阵** A∈Rn×n，并且存在一个非零向量 v，使得 Av\=λv，其中 λ 是一个标量，那么这个标量 λ 就是矩阵 A 的一个**特征值**。矩阵作用于特征向量时，特征向量的方向保持不变，只是其大小发生变化（被特征值λ缩放）
- **奇异值（Singular Value）** ：适用于所有矩阵。如果一个矩阵 A∈Rm×n，则其奇异值是通过对 A进行奇异值分解（SVD）得到的。奇异值是矩阵的**伸缩因子**，描述了矩阵在不同方向上的伸缩，奇异值是矩阵的**非负实数**，它们是矩阵 A 的矩阵 ATA 或 AAT 的特征值的平方根

  $$
  A=U\Sigma V^T
  $$

  - U∈Rm×m 和 V∈Rn×n 是正交矩阵，Σ 是一个对角矩阵，其对角线上的元素为矩阵 A 的奇异值

## Gram矩阵

$$
G = XX^T
$$

假设X形状为`(num_channels, n)`（即每一行是一个通道的特征向量），则G的形状为`(num_channels, num_channels)`，其中每个元素 `G[i][j]` 是第 `i` 个通道和第 `j` 个通道的特征向量的内积，表示不同通道特征之间的相关性

# 概率论

## Non-IID

非独立同分布数据

**独立同分布（IID）假设**：假设数据样本独立且来自同一分布，即

$$
P(x_i,y_i)=P(x_j,y_j)
$$

且样本间无相关性

Non-IID的三种情况：

- **不独立但同分布**：例如时间序列数据，样本间存在时序相关性
- **独立但不同分布**：如不同地区用户的数据因地域差异导致分布不同
- **既不独立也不同分布**：联邦学习中常见，客户端数据因用户偏好或环境差异呈现复杂异构性

# 信息论

## 熵 H(X)

随机变量 X 的熵（Entropy）表示 X 的不确定性或信息量，熵越大，随机变量的结果越不确定

假设 X 取值为 {x1,x2,…,xn}，其对应的概率分布为 P(x1),P(x2),…,P(xn)，则：

$$
H(X) = - \sum_{i=1}^{n} P(x_i) \log P(x_i)
$$

其中log通常以 2 为底，表示信息的比特（bit），也可以使用自然对数

如果所有可能的结果都具有相等的概率，那么熵会达到最大值，表示最不确定的情况

交叉熵：用于衡量两个概率的区别，$H(p, q) = \sum_{i} -p_ilog(q_i)$  

## KL-散度

$D_{KL}(P \| Q)$: P 和 Q 的 KL‐散度，衡量两个概率分布 P 和 Q 之间差异, 表示使用分布 Q 来近似分布 P 时所带来的额外信息量

$$
D_{KL}(P \parallel Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$

- 衡量 **从 P 到 Q** 的信息损失，如果 P 和 Q 完全相同，$D_{KL}(P||Q)$ 将为 0
- KL 散度不对称，即 $D_{KL}(P∥Q)\neq D_{KL}(Q∥P)$，因此它并不是一个真正的距离度量

## 互信息

Mutual Information，MI，衡量了一个变量的知识能够减少另一个变量不确定性的多少（两个变量之间的 **依赖性**），不仅反映了两者之间的相关性，还能揭示**非线性关系**

对于离散的随机变量 X 和 Y，它们的互信息$I(X;Y)$ 定义为：

$$
I(X;Y)=∑_{x∈X}∑_{y∈Y}p(x,y)log⁡(p(x,y)p(x)p(y))
$$

其中：

- p(x,y) 是 X 和 Y 联合分布的概率密度
- p(x) 和 p(y) 是 X 和 Y 的边缘分布的概率密度

直观理解：衡量知道 X 的值能够减少多少关于 Y 的不确定性

**性质：**

- **对称性**：互信息是对称的，即 I(X;Y)\=I(Y;X)。这意味着 X 和 Y 之间的互信息量是一样的，不管哪个变量先被观测
- **非负性**：互信息总是大于或等于零，即 I(X;Y)≥0。当 X 和 Y 完全独立时，互信息为 0
- **最大值**：如果 X 和 Y 完全相关（即它们的联合分布完全由其边缘分布决定），则互信息取最大值

‍

‍

‍

# 优化算法

## Momentum

动量技术，一种加速梯度下降法，用来解决标准梯度下降中 **收敛速度慢、容易卡在局部最小值或鞍点** 的问题

两个核心步骤：

1. **更新动量变量：**

    $$
    v_t=\beta v_{t-1}+(1-\beta)\nabla \theta_t
    $$

    - 𝑣𝑡：当前的速度/移动方向（动量），𝛽：动量系数（通常取 0.9），∇𝜃𝑡：当前梯度
    - 初始𝑣0\=0，不断展开得：

      即：**当前动量是所有历史梯度的指数加权平均**，参数更新时**引入了之前梯度的“累积”方向**，使得更新方向更稳定、加速收敛

      $$
      v_t = (1-\beta)\sum^{t}_{i=1}\beta^{t-i} \nabla \theta_i
      $$
2. **用动量更新参数：**

    $$
    \theta_{t+1}=\theta_t - \alpha v_t
    $$

    - 𝜃：模型参数，𝛼：学习率

```python
# momentum参数，即β
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

## **RMSProp**

Root Mean Square Propagation，常用 **自适应学习率优化算法**， **AdaGrad** 的改进版本，解决 AdaGrad 在训练过程中学习率急剧下降的问题

原理：对于每个参数，维持一个加权的平方梯度的滑动平均，以便动态调整该参数的学习率

1. **计算梯度平方的指数加权平均：**

    $$
    v_t = \beta v_{t-1} +(1-\beta)\nabla \theta^2_t
    $$

    $v_t$是第 t 次迭代时梯度平方的加权平均，β 是一个衰减因子（通常设为 0.9），控制历史梯度的影响程度
2. **更新参数：**

    $$
    \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{v_t+\epsilon}}\nabla\theta_t
    $$

    α ：学习率，ϵ 是一个非常小的常数（通常设为 10−8）用来防止除零错误，∇θt 是当前梯度

```python
# 创建 RMSProp 优化器
optimizer = torch.optim.RMSprop(
    model.parameters(), 
    lr=0.01, 
    alpha=0.9, 
    eps=1e-8
)
```

优化器的比较：

|特性/优化器|**SGD**|**Momentum**|**AdaGrad**|**RMSProp**|
| -------------| ------------------| --------------------| ----------------| --------------------|
|学习率调整|固定|动量调整|自适应学习率|自适应学习率|
|主要优点|简单，计算高效|加速收敛，稳定性好|适应稀疏数据|防止学习率急剧下降|
|适用场景|适用于大规模数据|适用于大规模数据|适用于稀疏数据|适用于非平稳目标|

## Adam

Adaptive Moment Estimation，结合 **Momentum** 和 **自适应学习率（如 AdaGrad）**  的思想，训练速度快，收敛效果好，适合处理**稀疏梯度**和**噪声大**的情况

Adam 优化器对每个参数都维护了：两个量，使用这两个量来动态调整每个参数的学习率

- 一阶矩（均值）：**梯度的趋势**（像 Momentum），公式：

  $$
  m_t = \beta _1 m_{t-1} +(1-\beta_1)\delta_t
  $$
- 二阶矩（方差）——表示 **梯度的大小波动**（像 RMSProp），公式：

  $$
  v_t = \beta_2v_{t-1}+(1-\beta_2)\delta_t^2
  $$
- 偏差修正：

  $$
  \hat m_t = \frac{m_t}{1-\beta_{1}^t} \\
  \hat v_t = \frac{v_t}{1-\beta^t_2}
  $$
- 更新参数：

  $$
  \theta_t=\theta_{t-1}-\alpha\cdot\frac{\hat m_t}{\sqrt{\hat v_t} +\epsilon}
  $$

解释：

- **β1 和 β2**：控制**记忆程度**（衰减系数）

  - β1：控制**一阶矩**（即梯度的平均趋势）更新时**保留过去梯度的比例**，通常设为 `0.9`​，意味着保留 90% 的过去梯度趋势
  - β2：控制**二阶矩**（即梯度平方的平均）更新时**保留过去梯度平方的比例**，通常设为 `0.999`​，意味着过去的梯度波动被记得更久

  可以理解为：β 越大，越“念旧”，对过去的记忆越强；越小，越“健忘”，更关注当前的梯度
- **α**：学习率（`lr`​），控制每一步走多大，太大可能震荡，太小又学得慢，默认 `alpha = 0.001`​
- **θ** ：要学习的参数
- **ϵ**：防止除零的小数，一般设为 `1e-8`​，也可以让算法更数值稳定

```python
optimizer = torch.optim.Adam(
    params,
    lr=0.001,           # 学习率
    betas=(0.9, 0.999), # β1 是一阶矩动量衰减因子，β2 是二阶矩
    eps=1e-8,           # 为了防止分母为 0 而加的小常数
    weight_decay=0,     # L2 正则化项
)

for epoch in range(num_epochs):
    optimizer.zero_grad()
    loss = loss_fn(model(input), target)
    loss.backward()
    optimizer.step()
```

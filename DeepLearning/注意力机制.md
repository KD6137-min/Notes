# 注意力机制

缩放点积注意力：Scaled Dot-Product Attention，Transformer 架构核心组件之一，用于计算序列中元素之间的关系和重要性

- 核心思想：通过计算查询向量（Query）、键向量（Key）和值向量（Value）之间的关系，来决定如何从值向量中加权组合出一个输出向量
- 具体步骤：

  - 三个输入矩阵：

    - **Query（Q）** ：表示当前要关注的元素
    - **Key（K）** ：表示所有元素的特征
    - **Value（V）** ：与 Key 相对应的元素内容
  - **计算点积**：对每个 Query 向量与所有 Key 向量进行点积，得到一个表示相关性或注意力分数的矩阵
  - **缩放**：将上述分数除以一个缩放因子，通常是 Key 向量的维度的平方根。这是为了防止在计算 softmax 时，点积结果过大导致梯度消失的问题
  - **Softmax**：对归一化后的分数应用 softmax 函数，得到注意力权重，表示各个 Key 对当前 Query 的重要性
  - **加权求和**：用计算得到的注意力权重对 Value 向量进行加权求和，最终得到输出
- 数学表达

  $$
  Attention(Q, K, V) = softmax( \frac{QK^T}{\sqrt {d_k}}) V
  $$

‍

输入 E（每个词的向量）**通过三个不同的线性变换矩阵**生成查询（Query）、键（Key）、值（Value）矩阵:

$Q = EW^Q, K = EW^K, V = EW^V$

这里的W都是可以**学习的参数矩阵**，作用就是从原始表示 E 中学习出适合注意力机制使用的 Q、K、V 表示

$$
head_i = Attention(EW_iQ, EW_iK, EW_iV)
$$

变成

$$
head_i = Attention(QW_iQ, KW_iK, VW_iV)
$$

Q、K、V 只是中间变量，怎么表示都可以，只要数学等价

- 在 **Encoder 自注意力**中，Q\=K\=V\=E
- 在 **Encoder-Decoder attention**中，Q\=decoder hidden，而 K,V\=encoder output

# 循环神经网络

时序模型中，当前数据与之前观察到的数据相关

自回归模型：使用自身过去数据来预测未来

统计工具：在时间t观察到$x_t$，则得到T个不独立的随机变量$(x_1,\ldots,x_T) \sim p(x)$
$$
p(x) = p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1,x_2) \cdot \ldots p(x_T|x_1,\ldots x_{T-1})
$$
两种方案：

- 马尔科夫假设：假设当前数据只跟$\tau$个过去数据点相关
  $$
  p(x_t|x_1,\ldots x_{t-1})=p(x_t|x_{t-\tau}, \ldots x_{t-1})=p(x_t|f(x_{t - \tau}, \ldots x_{t-1}))
  $$
  例如在过去数据上训练一个MLP模型

- 潜变量模型：引入潜变量$h_t$来表示（概括）过去信息，$h_t=f(x_1,\ldots x_{t-1})$，则$x_t=p(x_t|h_t)$

  ![image-20250624224024822](./../assets/image-20250624224024822.png)



文本预处理：将整个文本作为一个**时序数据**



![image-20250627205624139](./../assets/image-20250627205624139.png)

## LSTM

Long Short-Term Memory，一种改进的RNN，解决传统RNN在处理长序列时易遗忘早期信息、难保留长期依赖、易梯度消失/爆炸的问题，内部维持**细胞状态”（cell state）** ，像“传送带”一样让信息流动

![截屏2025-05-06 12.57.18](../assets/截屏2025-05-06%2012.57.18-20250605150331-kqintud.png)

核心：**门机制(gate)** ：每个时间步比普通 RNN 多了三个**门**（让信息选择通过的方法）

- **遗忘门（Forget Gate）** ：决定“上一个状态中的信息”，要**忘记多少**，由𝑓𝑡和𝐶𝑡−1计算决定

  ![截屏2025-05-06 10.06.27](../assets/截屏2025-05-06%2010.06.27-20250605150108-14mk5o5.png)![截屏2025-05-06 10.07.05](../assets/截屏2025-05-06%2010.07.05-20250605150114-x3d06wc.png)

  $$
  遗忘门：f_t = \sigma (W_f \cdot [h_{t-1},x_t]+b_f)
  $$

  𝜎为sigmoid函数，𝑥𝑡为当前输入，ℎ𝑡−1为上一个时间步的隐藏状态，𝐶𝑡为当前cell状态

- **输入门（Input Gate）** ：决定当前时间步的输入，要**更新哪些信息**![截屏2025-05-06 12.50.11](../assets/截屏2025-05-06%2012.50.11-20250605150137-1spidza.png)
$$
  输入门：i_t= \sigma (W_i \cdot [h_{t-1},x_t]+ b_i) \\
  候选值：\tilde C_t = tanh(W_C\cdot [h_{t-1},x_t] + b_C) \\
  细胞状态更新：C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde C_t
$$

$i_t$为要保留下来的新信息，𝐶𝑡为新数据形成的控制参数/细胞状态
- **输出门（Output Gate）** ：决定**当前隐状态**要输出什么内容![截屏2025-05-06 13.16.35](../assets/截屏2025-05-06%2013.16.35-20250605150215-roxnf0p.png)
  $$
  输出门：o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\\
  当前隐藏状态：h_t = o_t \cdot tanh(c_t)
  $$

Pytorch应用：

```python
import torch.nn as nn

lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

# 假设输入：batch_size=3, seq_len=5, input_size=10
input = torch.randn(3, 5, 10)
output, (hn, cn) = lstm(input)
```

## GRU

Gated Recurrent Unit，RNN 的一种变体，轻量级LSTM ，**解决普通 RNN 无法捕捉长期依赖、梯度消失等问题**，但结构更简单、计算更高效（去掉了cell state，合并了一些门结构）

|结构组成|LSTM|GRU|
| ----------------| ----------------------------| -------------------------------|
|门控数量|3 个门（遗忘、输入、输出）|2 个门（重置、更新）|
|独立 cell 状态|有 cell state（`c_t`​）|没有，直接用 hidden state|
|参数量|多|少（更快）|
|性能|表现稳健，适合长序列|性能接近或优于 LSTM，效率更高|

核心门机制：

1. **更新门（Update Gate）** ：决定当前隐藏状态要保留多少上一次的记忆
2. **重置门（Reset Gate）** ：决定当前输入和上一个隐藏状态融合的程度

核心公式：

$$
更新门：z_t = σ(W_z · [h_{t-1}, x_t]) \\
重置门：r_t = σ(W_r · [h_{t-1}, x_t]) \\
候选隐藏状态：h̃_t = tanh(W_h · [r_t * h_{t-1}, x_t]) \\
最终隐藏状态：h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t
$$

$z_t$更新门控制新旧记忆的“权重”，$r_t$重置门控制对旧记忆的“清除程度”

PyTorch应用：

```python
import torch.nn as nn

gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

input = torch.randn(3, 5, 10)  # batch=3, seq=5, input_size=10
output, hn = gru(input)
```

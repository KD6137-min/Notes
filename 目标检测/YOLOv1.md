# YOLOv1

You Only Look Once，一种端到端的单阶段目标检测方法

## before YOLO

主要是**两阶段方法**：候选区域生成 → 特征提取 → 分类 + 边框回归，计算量大、速度慢

**YOLO 的问题意识**：能不能把目标检测直接建模为一个**回归问题**，从图像像素一步到位预测目标的类别与位置

**地位：**第一个真正实现 **端到端单阶段检测（one-stage）** 的框架，把检测变成了一个统一的回归问题，极大提升了速度，推动了实时目标检测的发展



---



## 基本原理

1. **统一框架**：将检测作为一个整体的**回归问题**，从图像直接预测**边界框 + 类别概率 + 置信度**
2. **网格划分**：将输入图像划分为 $S \times S$ 网格，每个网格 cell 负责预测其内部物体的边界框与类别
3. **输出内容**：
    - $B$ 个预测框，每个框包含：位置坐标 $(x,y,w,h)$ + 置信度
    - 类别条件概率 $Pr(Class|Object)$
    - 最终得分：$Pr(Class|Object) \times Confidence$
4. **端到端训练**：单一神经网络联合优化所有任务，不再需要 proposal + SVM + 边框回归等分离步骤



------



## 论文结构

1. **引言**：传统检测速度慢、流程复杂，YOLO 的动机是简化检测 → 回归建模
2. **方法**：
    - 输入图像划分为网格
    - 网络结构：GoogLeNet 风格 backbone + 输出层
    - 输出：每个 cell 的 bbox + 类别概率
    - 损失函数设计（坐标误差、置信度误差、类别误差，带权重系数）
3. **实验**：在 PASCAL VOC 上评估，速度远快于 R-CNN 系列，同时精度也有竞争力；**消融实验**展示损失设计的重要性
4. **结论**：YOLO 提供了检测的一种新范式 —— 端到端的单阶段检测，为后续 YOLO 系列奠定了基础

## 创新点

放弃**漫无目的**的锚框生成，转而使用**有组织、有纪律**的网格化锚框预测

1. 第一次把检测建模为 **端到端单一回归问题**（像素 → bbox + class）
2. 统一框架：不再依赖 proposal、SVM、分离训练，检测作为一个整体函数近似
3. 实现了**实时检测**，速度大幅提升（比 R-CNN 系列快一个数量级）

总结：**YOLOv1 = Grid 划分 + 回归预测 (位置+置信度+类别)**

## 缺点

1. **定位精度差**：尤其对小物体
2. **容易漏检**：一个 cell 只能预测有限数量目标
3. **分类与定位耦合**：最终得分是类别条件概率 × IoU，分离性不足



---



## 网络设计

**设计思路**：把“有没有目标”**单独拿出来预测**，而不是直接分类

> 只有cell中真的有目标时，才会用其后 20 个输出计算分类误差，网络学到的就是“当有目标时应该输出怎样的类别概率分布”，所以这些概率就是在“有目标”的前提下学出来的 —— 也就是条件概率
>
> 若cell中根本没有目标，则类别概率无意义，会有大量假阳性，预测质量差

**网络组成**：

1. **特征提取网络（backbone）**：GoogLeNet风格的 24 conv + 2 FC，学习图像的空间+语义特征

    ![截屏2025-09-15 19.10.38](../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.10.38.png)

    *   边缘、纹理（浅层）
    *   局部形状（中层）
    *   语义信息、物体结构（深层）

    逐步将图像压缩成一个**高维语义特征图**，每个网格 cell 对应原图的一小块区域，携带了局部感受的信息

2. **目标检测头（输出层**）：网络最后几层，负责把特征图映射到**结构化的输出空间**（目标框 + 类别 + 置信度）



---



## 实现细节

### :o: 置信度

<img src="../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.24.44.png" alt="截屏2025-09-15 19.24.44" style="zoom:50%;" />

$Pr(Object)$：Cell中包含物体的概率，有则1无则0

- $confidence = Pr(Object) * IoU^{truth}_{pred}$
  
- $class scores = Pr(Class_i|Object)$

- 最终的类别置信度（计算得出，不在网络输出中）：$ClassConfidence_i = Pr(Class_i|Object) \times Confidence$，共7\*7\*2=98 个，作为最终分数，传入NMS等后续处理

### :o: 损失函数

<img src="../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.53.09.png" alt="截屏2025-09-15 19.53.09" style="zoom:50%;" />

> $\mathbb{1}^{obj}_i$：第 i 个网格是否包含物体中心点，是1否0
>
> $\mathbb{1}^{obj}_{ij}$：第 i 个网格的第j个bbox是否负责预测目标，是1否0
>
> $\mathbb{1}^{noobj}_{ij}$：第 i 个网格的第j个bbox是否负责预测目标，是0否1

**坐标损失** = 中心点定位误差 + 宽高误差（对宽高取根号，尽量消除框尺寸影响，否则损失函数倾向于调整大框）

损失之间的权衡：用参数$\lambda_{coord}$和$\lambda_{noobj}$控制，防止不包含物体的损失值过大，压倒了包含物体的梯度

> :warning: 注意
>
> ​	**yolo v1损失函数中带hat的符号解释**：一般情况下，加hat的通常表示模型的预测值，不加的值表示真实标签（ground truth），而YOLOv1原始论文中，采用了“非主流”写法，$p_i(c)$表示预测值，模型输出，而$\hat p_i(c)$表示真实值




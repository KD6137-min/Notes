# YOLOv1

## 特点

**统一框架：**

- **one-stage**，之前的RCNN都是两阶段
- 单一神经网络，端到端训练
- 回归代替分类，把检测任务作为一个统一的回归问题，直接回归 **分类+定位+置信度预测**

> 之前的方法流程：候选区域生成 + 分类器分类 + 边框回归器修正位置，检测被看作**分类问题 + 回归问题**两个分离的步骤

**缺点：**

- 定位精度差：小目标检测能力若
- 容易漏检
- 输出是类别概率*IOU，分离性不足

## 网络设计

### **设计思路**

**按网格顺序**告诉你：‘我在这里、这里、这里看到目标了’，你再去信任我的框和分类

YOLO 把“有没有目标”**单独拿出来预测**，而不是直接分类

> 只有cell中真的有目标时，才会用其后 20 个输出计算分类误差，网络学到的就是“当有目标时应该输出怎样的类别概率分布”，所以这些概率就是在“有目标”的前提下学出来的 —— 也就是条件概率
>
> 若cell中根本没有目标，则类别概率无意义，会有大量假阳性，预测质量差

### **网络组成**

1. **特征提取网络（backbone）**：Darknet，24个卷积层+2个FC层，学习图像的空间+语义特征

    ![截屏2025-09-15 19.10.38](../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.10.38.png)

    *   边缘、纹理（浅层）
    *   局部形状（中层）
    *   语义信息、物体结构（深层）

    逐步将图像压缩成一个“高维语义特征图”，每个网格 cell 对应原图的一小块区域，携带了局部感受的信息

2. **目标检测头（输出层**）：网络最后几层，负责把特征图转化为**结构化预测**，学的东西是**特征 -> 预测值的映射**（通过损失函数学习）

**总结：**YOLO 的深层网络学到的是从图像中提取有用的空间特征，并将这些特征映射到一个结构化的输出空间（即目标框 + 类别 + 置信度），通过端到端训练把整个“感知 → 理解 → 预测”过程融合为一个函数

### 置信度

<img src="../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.24.44.png" alt="截屏2025-09-15 19.24.44" style="zoom:50%;" />

- $confidence = Pr(Object) * IoU^{truth}_{pred}$
    - $Pr(Object)$：Cell中包含物体的概率，有则1无则0

- **class scores**：$Pr(Class_i|Object)$

- 最终的类别置信度（计算出来，不在网络输出中）：$ClassConfidence_i = Pr(Class_i|Object) \times Confidence$，共7\*7\*2=98 个，作为最终分数，传入NMS等后续处理

### 损失函数

<img src="../assets/%E6%88%AA%E5%B1%8F2025-09-15%2019.53.09.png" alt="截屏2025-09-15 19.53.09" style="zoom:50%;" />

$\mathbb{1}^{obj}_i$：第 i 个网格是否包含物体中心点，是1否0

$\mathbb{1}^{obj}_{ij}$：第 i 个网格的第j个bbox是否负责预测目标，是1否0

$\mathbb{1}^{noobj}_{ij}$：第 i 个网格的第j个bbox是否负责预测目标，是0否1

坐标损失 = 中心点定位误差 + 宽高误差

对宽高取根号：尽可能消除大尺寸框和小尺寸框之间的差异，否则损失函数会倾向于调整大的预测框

损失之间的权衡：用参数$\lambda_{coord}$和$\lambda_{noobj}$控制，防止不包含物体的损失值过大，压倒了包含物体的梯度



> :warning: 注意
>
> ​	**yolo v1损失函数中带hat的符号解释**：一般情况下，加hat的通常表示模型的预测值，不加的值表示真实标签（ground truth），而YOLOv1原始论文中，采用了“非主流”写法，$p_i(c)$表示预测值，模型输出，而$\hat p_i(c)$表示真实值






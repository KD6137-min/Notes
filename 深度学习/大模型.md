## Qwen3-VL 多模态大模型使用

对「图像 + 文本」输入进行对话式推理，生成文本输出

### 核心组件

- `model`：神经网络本体（参数+结构）

  ```python
  model = Qwen3VLForConditionalGeneration.from_pretrained(	# ForConditionalGeneration表示生成模型，支持.generate()
      "Qwen/Qwen3-VL-8B-Instruct",
      dtype="auto",		# 自动选择fp16 / bf16 / fp32
      device_map="auto"	# 自动选GPU，多卡则切层，显存不够则CPU+GPU混合
  )
  ```

  - 方法：`.generate()`，标准自回归生成，输入已有token，输出为继续往后预测token

    ```python
    generated_ids = model.generate(**inputs, max_new_tokens=128)
    # 把prompt部分裁掉，只保留模型生成的回答
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    ```

- `processor`：输入/输出的翻译官，功能有：文本 tokenizer、图像预处理、把对话结构整理成模型约定的格式（tensor）、把模型输出的 token转换为人能读的文本

  ```python
  processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")
  ```

  方法：

  - `.apply_chat_template()`，把 messages 转化为模型规定的 prompt 模板，插入 special tokens，tokenizer → token ids，转成 PyTorch Tensor

    ```python
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    )
    
    # 转变为
    """
    {
      "input_ids": ...,
      "attention_mask": ...,
      "pixel_values": ...   # 图像特征
    }
    """
    ```

  - `.batch_decode()`，把模型生成的 token id 序列 → 转换成人类可读的文本字符串

    ```python
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
    ```

- `messages`：Qwen 系列定义的**多模态对话协议**，一个`turn`（一个角色在某一时刻的完整发言），包含角色、内容，内容可混合image、text、video

  ```python
  messages = [
      {
          "role": "user",
          "content": [
              {"type": "image", "image": "..."},
              {"type": "text", "text": "Describe this image."},
          ],
      }
  ]
  ```

### 完整流程

```python
from modelscope import Qwen3VLForConditionalGeneration, AutoProcessor

model = Qwen3VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-VL-8B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```



### 🟡 Level 1：Prompt / 指令微调（不改权重）

- 改 prompt
- 改 messages 结构
- 改 system 指令

👉 **成本最低，首选**

------

### 🟠 Level 2：LoRA / PEFT（强烈推荐）

> **科研 & 工程中 90% 的“微调”都在这**

#### 思路

- 冻结原模型
- 只训练：
  - attention 的低秩矩阵
- 显存需求：
  - 8B 模型 ≈ 16–24GB 可跑

#### 框架

- `peft`
- `transformers + Trainer`
- `modelscope + swift`

#### 示例（概念级）

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)

model = get_peft_model(model, lora_config)
```

然后：

```python
loss = model(**inputs, labels=labels).loss
loss.backward()
```







## 1) 违规行为 4 类

### A. 静态可见物违规（单帧就能判）

- 未戴安全帽、未穿反光背心
- 进入禁入区域（有围挡/标线/警戒线）
- 消防通道堆物、材料乱堆放（可做但定义要清晰）
- 安全网缺失、洞口未盖板（通常需要分割/结构识别）

**方法**：目标检测 / 语义分割 + 区域规则

### B. 距离/接近类违规（需要“米”的概念）

- 靠近挖机/塔吊/吊装区
- 靠近高压线/危险边缘

**方法**：检测 + 跟踪 + 测距（几何/深度/RTK）+ 规则（5m 阈值、分级告警）

### C. 动作/时序类违规（单帧不够）

- 在设备运行范围内逗留
- 翻越围挡、逆行、追逐打闹
- 吊装下方停留、吊物下穿行

**方法**：检测 + 跟踪 + “停留时间/轨迹”规则
（可选：姿态估计/动作识别，但工程上常用规则就够了）

### D. 结构/环境类风险（更偏“发现隐患”）

- 高处临边未防护
- 脚手架缺杆/松动（难度高）
- 基坑边无围挡
- 火源/烟雾（可用火焰/烟雾检测，但误报要特别管）

**方法**：分割/结构识别 + 规则；火焰/烟雾可做专用检测器并加多帧确认

## 2) 无人机巡检做检测的“标准系统架构”

**强烈建议你按这个架构搭**（后期可扩展，不会越做越乱）：

1. **数据采集层**：航线规划 + 稳定取景
2. **感知层**：检测/分割/跟踪
3. **定位测距层**：把像素变成“区域/米”
4. **规则与告警层**：谁、在哪、多久、是否违规
5. **证据层**：截图/短视频片段 + 时间地点 + 违规类型
6. **回传与报表层**：地面站显示、日报/周报统计

> 交付时甲方最看重的往往是第 4–6 层：可解释、可复核、可追责

## 3) 你可以直接参考的“违规清单”与对应技术手段

下面这些是工地常见违规，你可以按优先级逐个上线：

### 优先级最高（ROI 大、最容易做）

1. **未戴安全帽**：检测人 + 头部/安全帽规则
2. **进入禁入区域**：人检测 + 区域多边形（ROI）
3. **靠近危险设备/作业区**：人/设备检测 + 测距/几何 + 规则
4. **高处临边无防护**：结构分割（围挡/护栏/洞口）+ 规则（难度中等）

### 次优先（需要时序/更难标注）

1. **吊装下方停留**：识别吊物/吊钩区域 + 跟踪 + 停留时间阈值
2. **人员聚集**：人检测 + 密度/聚类统计（可做“疑似聚集”提示）
3. **烟雾/明火**：专用检测器 + 多帧确认 + 置信度门控

## 4) 无人机巡检比固定摄像头更难的 5 个点

1. **视角变化大**：俯视/斜视导致外观分布变
2. **目标小**：人、帽子、警戒线都变小
3. **抖动/运动模糊**：影响检测和分割
4. **遮挡多**：设备、钢筋、脚手架
5. **地面不平、层级多（脚手架）**：导致“距离/区域判断”复杂

对应工程策略：

- 航线固定高度与角度（尽量“标准化视角”）
- 提高分辨率、裁剪 ROI 推理
- 加 tracking + 多帧投票，降低瞬时误报漏报
- 规则分级：预警/告警/确认三段式

## 5) 最重要的一点：别从“模型”开始，从“验收口径”开始

同一个“违规”，不同甲方口径完全不同。举例：

- “进入危险区”：危险区是谁画？按设备回转半径？按地面标线？
- “靠近 5 米”：是到设备外轮廓 5 米？还是到危险区边界 5 米？
- “未戴安全帽”：安全帽被遮挡算不算？戴着但颜色接近背景算不算？

工程上要做的是：

- 把每条违规写成**可执行规则**
- 输出**证据截图/短视频**
- 给出**可解释字段**：时间、位置、距离、连续帧数、置信度

## 6) 给你一个“最小可用版本（MVP）”路线（建议照着做）

**第 1 版（最快出效果）**

- 人 + 安全帽检测
- 人进入禁入区域（手工画 ROI）
- 设备周边危险区（手工画 ROI 或按设备 bbox 扩张）
- 多帧投票报警 + 自动截帧取证

**第 2 版（更像产品）**

- 加 tracking，记录“同一人”的违规持续时间
- 危险设备类别化：挖机/吊车/塔吊不同危险区形状
- 几何测距（如果你能拿到高度/姿态/内参）

**第 3 版（更高级）**

- 分割洞口/护栏/边缘防护
- 结构化报表：点位热力图、违规统计趋势



| Dataset                                 | Task           | Setting         | Description                                                  |
| --------------------------------------- | -------------- | --------------- | ------------------------------------------------------------ |
| laolao77/ViRFT_COCO                     | Detection      | -               | Includes all categories from COCO, with a total of 6k entries. |
| laolao77/ViRFT_COCO_base65              | Detection      | Open-vocabulary | Includes 65 base categories from COCO, with a total of 6k entries. |
| laolao77/ViRFT_COCO_8_cate_4_shot       | Detection      | Few-shot        | Includes 8 selected categories from COCO under a 4-shot setting. |
| laolao77/ViRFT_LVIS_few_shot            | Detection      | Few-shot        | Includes selected categories from the LVIS dataset under a few-shot setting. |
| laolao77/ViRFT_CLS_flower_4_shot        | Classification | Few-shot        | Includes 102 categories from the Flower102 dataset, with 4 images per category. |
| laolao77/ViRFT_CLS_fgvc_aircraft_4_shot | Classification | Few-shot        | Includes 100 categories from the FGVC-Aircraft dataset, with 4 images per category. |
| laolao77/ViRFT_CLS_car196_4_shot        | Classification | Few-shot        | Includes 196 categories from the Stanford Cars dataset, with 4 images per category. |
| laolao77/ViRFT_CLS_pets37_4_shot        | Classification | Few-shot        | Includes 37 categories from the Oxford-IIIT Pets dataset, with 4 images per category. |
| LISA dataset                            | Grounding      | -               | Reasoning-based grounding tasks.                             |
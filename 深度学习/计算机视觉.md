# 计算机视觉

# R-CNN

Region-based Convolutional Neural Network，区域卷积神经网络

基本原理：

1. **候选区域生成**：使用选择性搜索（Selective Search）算法从输入图像中提取约2000个候选区域（Region Proposals）。这些区域通过合并相似的像素块生成，覆盖图像中可能存在的目标
2. **特征提取**：每个候选区域被缩放到固定尺寸（如227×227像素），然后输入预训练的CNN（如AlexNet）提取特征。这一步骤将图像转换为高维特征向量（如4096维），用于后续分类和定位
3. **分类与边界框回归**：

    - **分类**：使用支持向量机（SVM）对每个候选区域的特征进行分类，判断其属于目标类别还是背景
    - **回归**：通过线性回归模型调整候选框的位置和大小，使其更精确地包围目标
4. **非极大值抑制（NMS）** ：对重叠的候选框进行筛选，保留置信度最高的检测结果，去除冗余框

缺点：

- 计算效率低：需对每个候选区域单独提取特征
- 训练复杂：需分阶段训练CNN、SVM和回归器

# YOLO

You Only Look Once，目标检测模型

|版本|发布年份|作者/团队|核心改进|特点|论文|
| ------| ----------| -----------------------| -----------------------------------------------------------------------------------------------------------------------| ---------------------------------------------------------------------| ------------------------------------------------------------------------------------------------|
|**YOLOv1**|2016|Joseph Redmon|将目标检测变为单个 CNN 回归问题（将图像分成网格，每个网格预测边界框和类别），一步完成预测|快，但精度低，不适合小物体|《You Only Look Once: Unified, Real-Time Object Detection》|
|**YOLOv2**|2017|Redmon|引入 Anchor Boxes、BatchNorm、使用更深的特征网络（Darknet-19）、支持多类别|性能高于v1|《YOLO9000: Better, Faster, Stronger》|
|**YOLOv3**|2018|Redmon|使用 ResNet风格的Darknet-53、支持多尺度预测（FPN）、使用logistic回归处理多标签分类|性能大幅提升，经典，使用广泛|《YOLOv3: An Incremental Improvement》|
|**YOLOv4**|2020|Alexey Bochkovskiy|集成多种工程技巧优化（数据增强：Mosaic、DropBlock，激活函数：Mish，使用CSPDarknet53、CIoU损失），精度和速度进一步提升|效果全面超越 YOLOv3|《YOLOv4: Optimal Speed and Accuracy of Object Detection》|
|**YOLOv5**|2020|Ultralytics（非官方）|模型分为 nano、small、medium、large 等级，用 Python 工程化封装良好，支持训练、测试、推理、部署一条龙|用 PyTorch 重写，工程化能力强，提供小中大等多种模型，工业界极其流行|无正式论文，由 Ultralytics 发布|
|**YOLOv6**|2022|美团（Meituan）|更小模型结构、更快推理速度、优化 NMS、训练策略|工业级优化，适合边缘设备、手机端等部署|[https://github.com/meituan/YOLOv6](https://github.com/meituan/YOLOv6)|
|**YOLOv7**|2022|Wang et al.|强调速度和精度的平衡，引入 E-ELAN模块、模块融入RepConv、动态标签分配|精度和速度都优于 YOLOv6、YOLOv5|《YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors》|
|**YOLOv8**|2023|Ultralytics（官方）|完全重构的框架，支持分类、检测、分割，多任务统一，支持ONNX/TensorRT部署|不再兼容 YOLOv5 的结构，用法简洁，代码开箱即用||





# 迁移学习

广义概念，指将源任务学到的知识迁移到目标任务

两种实现方式：

- 微调：Fine-tuning，指在**预训练模型**的基础上，通过**少量特定任务的数据**对模型参数进行进一步调整，使其适应新任务的需求

  - fine：精细的，细致的，指高精度调整；tune：调节，调校
  - 通常**复制**预训练模型的参数作为初始值，但会根据新任务的数据更新部分或全部参数。例如：

    - **全参数微调**：更新所有参数（计算成本高）
    - **高效微调（如LoRA）** ：仅更新少量新增参数（如低秩矩阵），冻结大部分原始参数
  - 迁移学习可能仅复用模型的部分结构（如特征提取层），而微调通常涉及参数更新
  - 优势：

    - **高效性**：避免从头训练，节省计算资源
    - **小数据适配**：在目标数据较少时仍能提升性能
    - **领域适配**：使通用模型（如GPT）专业化（如医疗文本生成）
- 特征提取：**固定预训练模型的前几层（如卷积层、嵌入层）作为通用特征提取器**，仅训练新添加的分类层（如全连接层）以适应目标任务

  - 核心逻辑是：

    - **底层特征通用性**：预训练模型的浅层（如CNN的卷积层）通常学习边缘、纹理等基础特征，这些特征在不同任务间具有普适性
    - **高层特征特异性**：深层网络学到的特征更偏向源任务，因此可能需要调整或替换
  - **实现步骤**

    - **选择预训练模型**：使用在大型数据集上预训练的模型作为特征提取器
    - **冻结特征提取层**：固定预训练模型的参数，仅保留其输出特征（如CNN的Flatten层输出或BERT的CLS向量）
    - **构建新分类器**：在提取的特征上添加新的可训练层（如全连接层、SVM）以适应目标任务
    - **训练与微调**：仅训练新添加的层，或对部分高层进行微调以提升性能
  - **注意事项**

    - **领域差异**：若源域与目标域差异过大（如自然图像→文本），特征提取可能失效，需结合领域自适应技术
    - **负迁移风险**：若特征无关，可能导致性能下降，需通过特征选择或对齐缓解

应用：

样式迁移：用两个pretrained模型提取图像的内容和风格特征，并将其迁移到新图像生成任务中

![image-20250611190842245](./../assets/image-20250611190842245.png)

```python
import torch
import torch.nn as nn
import torchvision
from d2l import torch as d2l
%matplotlib inline

d2l.set_figsize()

content_img = d2l.Image.open('img/flower.jpg')
d2l.plt.imshow(content_img)

style_img = d2l.Image.open('img/light.jpg')
d2l.plt.imshow(style_img)

rgb_mean = [0.485, 0.456, 0.406]
rgb_std = [0.229, 0.224, 0.225]

def preprocess(img, image_shape):  
    transforms = torchvision.transforms.Compose([
        torchvision.transforms.Resize(image_shape),  
        torchvision.transforms.ToTensor(),  
        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)
    ])  
    return transforms(img).unsqueeze(0)     # 添加了批次维度，为了兼容torch模型通常接受的输入格式


def postprocess(img):  
    img = img[0].to(rgb_std.device)     # 提取第一个（也是唯一一个）样本，还原形状
    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)  
    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))


pretrained_net = torchvision.models.vgg19(pretrained=True)

style_layers, content_layers = [0, 5, 10, 19, 28], [25]

net = nn.Sequential(*[
    pretrained_net.features[i] for i in range(max(content_layers + style_layers) + 1)
])

def extract_features(X, content_layers, style_layers):
    contents = []
    styles = []
    for i in range(len(net)):
        X = net[i](X)
        if i in style_layers:
            styles.append(X)
        if i in content_layers:
            contents.append(X)

def get_contents(img_shape, device):
    content_X = preprocess(content_img, img_shape).to(device)
    contents_Y = extract_features(  )
```

‍

# 转置卷积

Transposed Convolution，用于上采样，核心目的是通过特定的计算方式将输入特征图的尺寸放大，同时保留或重建空间信息

数学本质：并非卷积的逆运算，而是通过反向传播卷积的矩阵运算关系实现的结果的含义：

- **形状恢复**：转置卷积的输出尺寸通常大于输入，例如在语义分割中用于将压缩的特征图恢复到原始图像大小
- **数值关系**：输出值与输入通过卷积核的加权组合相关，但并非精确还原原始数据（仅形状相似）
- **可学习性**：转置卷积的核参数可通过训练优化，相比插值上采样能更好地适应任务需求

注意事项：

- **棋盘效应**：转置卷积可能导致输出出现网格状伪影，可通过后续卷积层缓解
- **效率问题**：因需填充大量零值，计算成本较高

# 双线性插值

Bilinear Interpolation，在二维规则网格上对双变量函数（如x和y）进行插值，其核心思想是通过在两个方向上分别执行一次线性插值，利用邻近的四个已知点的值计算未知点的值

插值步骤：

- **第一步（X方向插值）** ：在x方向对相邻的两个点进行线性插值，得到中间点R\_1和R\_2的值：

  $$
  f(R_1) = (1-u) \cdot f(Q_{11}) + u \cdot f(Q_{21}) \\
   f(R_2) = (1-u) \cdot f(Q_{12}) + u \cdot f(Q_{22})
  $$

  其中u为x方向的小数部分（u\=x−⌊x⌋），用于计算的四个点（*Q*11,*Q*12,*Q*21,*Q*22）**必须是相邻的四个像素点**，且构成一个规则的2×2网格，这是双线性插值算法的核心前提
- **第二步（Y方向插值）** ：在y方向对R\_1和R\_2插值，得到目标点P的值：

  $$
  f(P) = (1-v) \cdot f(R_1) + v \cdot f(R_2)
  $$

  其中v为y方向的小数部分（v\=y−⌊y⌋）

特点：

- **非线性特性**：双线性插值的结果是两个线性函数的乘积，整体呈二次特性，而非严格线性
- **图像处理**：在图像缩放、旋转等操作中，通过加权平均周围4个像素值生成新像素，避免锯齿但可能导致边缘模糊
- **计算效率**：相比高阶插值（如双三次插值），计算量较小，适合实时处理

局限性：

- **平滑效应**：高频信息可能受损，放大图像时细节变模糊
- **边缘处理**：对非连续区域（如锐利边缘）可能产生伪影

‍

# 当前SOTA

截止2025年5月

- **RF-DETR**：在COCO数据集上mAP突破60%，推理速度达实时（30+ FPS），支持多分辨率动态调整
- **YOLOv5_mamba**：通过Mamba架构增强小目标检测，在无人机航拍等场景中表现优异
- **D-FINE**：细粒度分布细化技术使COCO mAP达55.8%，适用于高精度需求场景

‍

## IoU

Intersection over Union，交并比，用于衡量目标检测或语义分割模型预测结果与真实标注之间重叠程度

$$
IoU = \frac{\text{交集面积}}{\text{并集面积}} = \frac{|A \cap B|}{|A \cup B|}
$$

其中，$A$ 为预测框（或预测区域），$B$ 为真实框（或真实区域）

**特性：**

- **取值范围**：$[0, 1]$，值越大表示预测与真实结果重合度越高，**1表**完全匹配（预测框与真实框完全重合），**0表**无重叠
- **尺度不变性**：不受目标大小影响，仅关注重叠比例

**局限性：**

- **无重叠时失效**：若两框不重叠，IoU\=0，无法反映距离远近
- **忽略形状差异**：相同IoU下，不同长宽比的框可能重合方式不同

**改进变体：**

GIoU、DIoU、CIoU等，通过引入中心点距离、长宽比等约束提升评估效果

‍

## COCO mAP

Mean Average Precision，目标检测任务中用于评估模型性能的核心指标，尤其在MS COCO数据集中作为标准评估方法，通过计算**多类别平均精度（AP）的均值**得到的综合指标

核心特点：

- **多IoU阈值评估**：不同于PASCAL VOC仅使用IoU\=0.5，COCO采用**10个IoU阈值（0.5至0.95，步长0.05）** 的AP平均值：

  $$
  \text{mAP} = \frac{1}{10} \sum_{IoU \in \{0.5, 0.55, \dots, 0.95\}} AP_{IoU}
  $$

  这种设计能更严格地评估模型在不同定位精度下的表现
- **类别平衡**：对数据集中所有类别的AP取平均，避免某些类别主导结果

计算流程：

1. **匹配预测框与真实框**：

    - 对每个预测框，计算其与所有真实框的IoU，若最高IoU≥阈值且类别正确，则标记为True Positive（TP），否则为False Positive（FP）
    - 未被匹配的真实框记为False Negative（FN）
2. **生成PR曲线**：

    - 按置信度降序排列预测框，逐行计算累积的Precision（$P = \frac{TP}{TP+FP}$）和Recall（$R = \frac{TP}{TP+FN}$）
    - 绘制Precision-Recall曲线，并对曲线进行平滑处理（插值法取右侧最大Precision）
3. **计算AP**：AP为平滑后PR曲线下的面积（AUC），通常采用101点插值法近似计算
4. **汇总mAP**：对所有类别的AP取平均，并进一步对10个IoU阈值的AP取平均

COCO提供细分评估维度：

- **不同目标尺度**：`AP@small`​（面积\<32²像素）、`AP@medium`​（32²–96²）、`AP@large`​（\>96²）
- **严格指标**：`AP@IoU=0.75`​（高定位精度要求）
- **召回率（AR）** ：限制每张图像的检测框数量（如1/10/100个）下的平均召回率

‍

## 锚框法

可类比为“先撒网再调整”的钓鱼策略：

1. **撒网（生成锚框）：** 在图像上密密麻麻地生成各种形状的“虚拟框”（锚框），比如不同大小、长宽比的框，覆盖所有可能的位置，锚框是“猜测”的起点，模型通过大量样本学习如何修正这些猜测

    - **锚框是预定义的：** 锚框的生成基于固定规则（如缩放比s、宽高比r），与具体图像内容无关
2. **标记渔获（标注锚框）：** 计算每个锚框和真实目标框（人工标注的物体位置）的重叠程度（交并比IoU）

    - **正样本**：与真实框高度重叠的锚框，标记为“有鱼”（物体类别），并记录真实框相对于锚框的偏移量（如中心点位置、宽高的调整值）
    - **负样本**：完全不重叠的锚框，标记为“没鱼”（背景），训练时大量锚框是背景，只有少数参与物体检测，这是为了平衡正负样本
3. **训练模型：** 让模型学习两类任务：

    - **分类**：通过锚框与真实框的匹配（IoU计算）判断锚框里是“鱼”（某类物体）还是“水”（背景）
    - **回归**：预测如何调整锚框的位置和大小，让它更贴近真实框（比如往左移10像素，宽度扩大1.2倍），学习的偏移量为：

      - **中心点偏移**：Δx\=(x真实−x锚框)/w锚框，Δy同理
      - **宽高缩放**：Δw\=log(w真实/w锚框)，Δh同理，这种相对修正比直接预测坐标更稳定，尤其适合位置多变的目标

      通过大量样本的训练，模型会捕捉到**常见目标的尺寸和位置分布**（如“猫通常需要扩大锚框”），这种统计规律性使得模型能泛化到新图像
4. **优化筛选（预测阶段）：非极大值抑制（NMS），** 模型预测时会产生大量重叠的锚框，保留最准的一个，去掉冗余的（类似挑出网里最大的鱼，扔掉小鱼）

    - NMS：Non-Maximum Suppression，用于去除冗余的检测框的标准方法，保留最准确的预测结果，核心是 IoU 计算和贪心策略，改进版（如 Soft-NMS）可提升密集目标的检测效果

      - **核心原理：**

        - **输入**：一组检测框（Bounding Boxes）及其置信度（Scores）
        - **步骤**：

          - **按置信度排序**：从高到低排列所有检测框
          - **选取最高分框**：保留当前置信度最高的框，并计算其与剩余框的 IoU（交并比）
          - **抑制重叠框**：若 IoU 超过阈值（如 0.5），则删除该框（认为它们是同一目标）
          - **迭代处理**：重复上述步骤，直到所有框被处理完毕
      - **数学公式**：

        - IoU计算：

          $$
          \text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
          $$
        - **NMS 阈值**：通常设为 0.5（目标检测）或 0.7（人脸检测）
      - 代码实现：

        ```python
        # pytorch内置NMS
        import torchvision.ops as ops
        keep = ops.nms(boxes, scores, iou_threshold)
        
        # numpy实现
        import numpy as np
        
        def nms(boxes, scores, iou_threshold=0.5):
            # boxes: [N, 4] (x1, y1, x2, y2)
            # scores: [N]
            indices = np.argsort(scores)[::-1]  # 按置信度降序排序
            keep = []
            
            while len(indices) > 0:
                current = indices[0]
                keep.append(current)
                
                # 计算当前框与其他框的 IoU
                x1 = np.maximum(boxes[current, 0], boxes[indices[1:], 0])
                y1 = np.maximum(boxes[current, 1], boxes[indices[1:], 1])
                x2 = np.minimum(boxes[current, 2], boxes[indices[1:], 2])
                y2 = np.minimum(boxes[current, 3], boxes[indices[1:], 3])
                
                overlap = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)
                area_current = (boxes[current, 2] - boxes[current, 0]) * (boxes[current, 3] - boxes[current, 1])
                area_others = (boxes[indices[1:], 2] - boxes[indices[1:], 0]) * (boxes[indices[1:], 3] - boxes[indices[1:], 1])
                iou = overlap / (area_current + area_others - overlap)
                
                # 保留 IoU 小于阈值的框
                indices = indices[1:][iou < iou_threshold]
            
            return keep
        ```
      - 改进方法：

        - **Soft-NMS**：不直接删除重叠框，而是降低其置信度（避免误删）
        - **DIoU-NMS**：考虑框的中心距离，优化 IoU 计算
        - **Cluster-NMS**：加速 GPU 并行计算
      - **局限性：**

        - **阈值敏感**：IoU 阈值设置影响检测精度
        - **计算效率低**：传统 NMS 难以并行化

**动态适应能力：** 现代方法（如动态锚框生成、多尺度融合）能根据图像内容自动调整锚框参数，进一步适应不确定的物体位置，例如通过注意力机制聚焦目标关键区域生成锚框

核心：**预定义规则生成锚框，并通过模型学习如何调整这些锚框以适应不同图像中的目标**

‍

## ROI池化层

Region of Interest Pooling，用于将不同尺寸的候选区域（ROI）转换为固定大小的特征向量，以便后续分类和回归任务处理

**原理：**

1. **输入与坐标映射**

    - 输入包括特征图（来自CNN）和一组ROI坐标（$(x1, y1, x2, y2)$）
    - ROI坐标通过缩放（如`spatial_scale`​）映射到特征图上
2. **固定尺寸输出：** 每个ROI区域被划分为固定数量的子区域（如$7 \times 7$网格），对每个子区域执行**最大池化**或平均池化，生成统一尺寸的输出（如$7 \times 7 \times C$）
3. **作用**

    - 解决目标尺寸不一的问题，使全连接层能处理标准化特征
    - 提升计算效率，避免直接调整ROI尺寸导致的信息损失

**代码实现：**

```python
import torch
import torchvision.ops as ops

# 定义ROI池化层
class ROIPooling(nn.Module):
    def __init__(self, output_size):
        super().__init__()
        self.output_size = output_size  # 如 (7, 7)

    def forward(self, feature_map, rois):
        # feature_map: (N, C, H, W), rois: (num_rois, 5) [batch_idx, x1, y1, x2, y2]
        return ops.roi_pool(feature_map, rois, self.output_size)

# 示例
feature_map = torch.randn(1, 512, 28, 28)  # 特征图
rois = torch.tensor([[0, 0, 0, 10, 10], [0, 5, 5, 15, 15]])  # 两个ROI
pooling = ROIPooling(output_size=(7, 7))
output = pooling(feature_map, rois)  # 输出形状: (2, 512, 7, 7)
```

**关键点：**

- **坐标映射**：ROI坐标需根据特征图尺寸调整（如除以`spatial_scale`​）
- **池化操作**：子区域划分时，若无法整除则取整（如`floor`​或`ceil`​）

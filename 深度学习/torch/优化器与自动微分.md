# 三. 优化器（torch.optim）

参数优化算法实现

**常用优化器**：

- `optim.SGD`​, `optim.Adam`​, `optim.RMSprop`​
- 学习率调度器：`optim.lr_scheduler.StepLR`​

# 四. 自动微分（torch.autograd）

动态计算图与梯度计算

**关键方法**：

- `requires_grad=True`​ 启用梯度追踪
- `.backward()`​ 反向传播
- `torch.no_grad()`​ 禁用梯度

## Variable废弃

早期用于包装一个张量，以启用自动求导功能，现已和Tensor功能合并

## backward()

根据某个**标量**输出，自动计算标量对所有涉及的参数的梯度，并将梯度传递回模型的每一层，帮助优化参数。通常在损失函数上调用，更新网络的参数

```python
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x**2 + 2*x
loss = y.sum()
loss.backward()
print(x.grad)
```

> 注意：必须在标量张量上调用

两个参数：`y.backward(torch.ones_like(x), retain_graph=True)`

- gradient：指定反向传播的“权重因子”，非标量进行梯度计算时将输出加权为标量
- retain_graph：保留计算图，允许多次调用`backward()`而不报错，梯度会累加到.grad中
    - PyTorch默认在单次反向传播后自动释放计算图以节省内存，二次传播会报错

### **​`net.freeze()`​** ​ **：手动冻结层**

冻结模型的部分层（即不计算该层的梯度），避免对某些层进行更新，通常用于 **微调**

## 梯度相关上下文管理器和函数

### `torch.no_grad()`​：

用于在特定代码块中**临时**禁用梯度计算，可显著减少显存占用（避免存储中间梯度）并提升计算效率

#### **1. 工作机制**

1. 保存当前梯度状态：`torch.is_grad_enabled()`​
2. 关闭梯度计算：`torch.set_grad_enabled(False)`​
3. 执行代码块
4. 恢复原状态：通过 `finally`​ 恢复原状态

#### **2. 注意事项**

- 支持嵌套，内层上下文不会覆盖外层状态，但梯度计算始终处于关闭状态
- 通常结合 `model.eval()`​ 使用
- 退出上下文后，梯度计算自动恢复，无需手动干预
- 在上下文中，原地修改仍会执行，但不会记录梯度

#### **3. 与其他方法对比**

| **方法**                        | **作用范围** | **适用场景**                     |
| ------------------------------- | ------------ | -------------------------------- |
| `torch.no_grad()`​               | 代码块       | 临时禁用梯度（推荐用于局部操作） |
| `@torch.no_grad()`​              | 整个函数     | 装饰器形式，适用于推理函数       |
| `torch.set_grad_enabled(False)`​ | 全局或局部   | 需手动恢复状态，灵活性较低       |

### `torch.enable_grad()`​

强制启用梯度计算（即使外层有 `no_grad()`​）

### `torch.autograd.grad()`​

直接计算输出对输入的梯度

参数：

- `outputs`​：待求导的目标张量
- `inputs`​：需要计算梯度的输入张量
- `grad_outputs`​：权重向量（用于调整梯度方向）

### `torch.inference_mode()`​

专用于推理场景，提供比no\_grad()更高效的运行时性能，优化了张量的内存管理和计算

Pytorch1.10版本以上优先考虑使用`torch.inference_mode()`​

### `Tensor.detach()`

`Tensor`会记录计算图，每个 `Tensor` 内部都有一个 `grad_fn` 属性，指向生成它的运算节点，反向传播时从 `loss` 开始，沿着 `grad_fn` 链回溯，逐层计算梯度

调用`x.detach()`会返回一个 **新张量**，它和 `x` **共享相同的数据内存**，但是 **切断了计算图的追踪**，即返回的 `Tensor` **数据区指向原数据**（共享存储，不拷贝），所以效率高，但它的 `grad_fn = None`，也就是 **不再关联到原计算图**，这样从它出发做的运算，都会构建一个 **新的子计算图**，不会影响到原来的图

**几个易混淆的操作：**

- **`tensor.detach()`**：切断计算图，数据共享
- **`tensor.clone()`**：复制一份数据，仍然保留计算图关系
- **`tensor.detach().clone()`**：复制数据且切断计算图（最常见的安全用法）
- **`.data`（旧用法）**：早期 PyTorch 用 `.data` 来绕过 autograd，现在不推荐，因为容易出错


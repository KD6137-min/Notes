# 二. 神经网络构建（torch.nn）

## 卷积层

`nn.Conv2d()`：自动初始化权重(He初始化)



## 参数管理

- `.parameters()`​：返回所有**可训练参数**（当前模块和子模块中所有参数的扁平**迭代器**），**不按模块层级区分**，通常是 **​`torch.nn.Parameter`​**​ 类型的张量，包括权重、偏置等，用于**优化器传参**， 只包含模型中需要梯度的参数，不能直接保存/加载模型结构/状态，可对特定子模块单独调用 `.parameters()`​，分模块训练需用 `named_parameters()`​，通过字符串 `name`​ 判断参数属于哪个子模块
- `.named_parameters()`​：返回模型中**所有可训练参数**的名字和参数本身的迭代器
- `.named_children()`​：返回模块的**直接**子模块（带名字）
- `.named_modules()`​：返回模型中**所有**子模块及其名称
- `.children()`：返回当前模块的第一个子模块
- `.modules()`：递归地返回所有子模块的**生成器**
    - `._modules`：内部属性，**有序字典**，用于维护子模块的添加顺序，通常不由用户使用
    - `add_module(name, module)`：将子模块添加到模型的 `_modules` 字典，无返回值，`name` (str)子模块的名称，将用于在模型的 `__setattr__` 方法中注册子模块，并作为键存储在模型的 `_modules` 字典中；`module` (`nn.Module`)要添加的子模块
- `.state_dict()`​：获取所有模型的“状态信息”，返回一个 **字典（**​**​`OrderedDict`​**​ **）** ，键是参数或缓存变量的名字，值是张量，包含所有权重和偏置、有些非训练参数（如 BatchNorm 的 running\_mean 和 running\_var），常用于：
- **保存模型参数：**  `torch.save(model.state_dict(), "model.pth")`​
- **加载模型参数：**  `model.load_state_dict(torch.load("model.pth"))`​

| 特性             | `.parameters()`​                 | `.state_dict()`​                         |
| ---------------- | ------------------------------- | --------------------------------------- |
| 返回类型         | 迭代器，元素是`Parameter`​       | 有序字典，key 是名字，value 是张量      |
| 包含内容         | 可训练参数（需要梯度）          | 所有参数 + 缓存（如 BN 层的均值、方差） |
| 用途             | 通常用于传给优化器              | 通常用于保存、加载模型状态              |
| 是否包含名字     | ❌（除非用`named_parameters()`​） | ✅（key 就是参数名）                     |
| 是否包含缓存参数 | ❌                               | ✅（如`running_mean`​,`running_var`​）     |

`nn.Conv2d`有属性`kernel_size`



初始化

`nn.init.normal_()`

`nn.init.constant_()`

`nn.init.zeros_()`

`nn.init.xavier_uniform_()`

## 损失函数

- `nn.CrossEntropyLoss()`​：输入必须是未归一化的原始logits（模型最后一层的直接输出），内部集成Softmax和对数计算（稳定性技巧）

    参数`reduction`​：决定如何对**批量（batch）中每个样本的损失值**进行汇总，共有三个选项：

    - `'mean'`​：返回所有样本损失的平均值
    - `'sum'`​：返回所有样本损失的总和
    - `'none'`​：不进行任何聚合，直接返回每个样本的独立损失值（形状为 `[batch_size]`​）

- `nn.BCEWithLogitsLoss()`​：用于二分类任务的损失函数, Binary Cross Entropy with Logits Loss，意为带有 logit 输入的二元交叉熵损失函数

    - Logits：没有经过 Sigmoid 的原始输出分数（通常是任意实数），不是概率

- `nn.MSELoss()`​：

‍

## 模型的行为模式

### **​`net.train()`​** ​ **：训练模式**

- **​`Dropout`​**​：训练时Dropout 会随机 **丢弃** 一部分神经元，防止过拟合
- **​`BatchNorm`​**​：BatchNorm 层使用 **当前批次的统计信息**（即当前批次的均值和方差）进行标准化

### **​`net.eval()`​** ​ **：评估模式/推理模式**

- **​`Dropout`​**​：在评估模式下，Dropout 层会 **不丢弃任何神经元**，即使用 **所有神经元**
- **​`BatchNorm`​**​：在评估模式下，BatchNorm 层会使用 **训练期间计算的全局均值和方差**，而不再依赖当前批次

‍

# 概述

`torch.nn`神经网络库，子模块分为 **核心层**、**容器**、**损失函数**、**工具类** 和 **特殊功能**





## 类继承关系

```plaintext
nn.Module（基类）
├── nn.Sequential
├── nn.ModuleList
├── nn.ModuleDict
├── 各网络层（Conv2d, Linear, ...）
├── 损失函数（MSELoss, ...）
└── 工具类（Dropout, ...）
```

- **所有层和容器均继承自 `nn.Module`**，因此支持参数管理、GPU迁移和模型保存
- **函数式接口**（`nn.functional`）提供无状态操作，与 `nn.Module`互补

## `.Module`基类

所有网络的基类，可以包含其它`Modules`,允许使用树结构嵌入他们，可将子模块赋值给模型属性

自定义网络类必须重写`forward()`

- `.add_module(name, module)`：被添加的module可通过`name`属性获取

- `.children()`：返回当前模型子模块的**迭代器**

    ```python
    for sub_module in model.children():
        print(sub_module)
    ```

- `.modules()`：返回当前模型所有模块的迭代器

# 核心层

| 子模块             | 功能描述                   | 示例类/函数                  |
| ------------------ | -------------------------- | ---------------------------- |
| **卷积层**         | 处理空间数据（图像、视频） | `Conv1d`, `Conv2d`, `Conv3d` |
| **循环神经网络层** | 处理序列数据               | `RNN`, `LSTM`, `GRU`         |
| **线性层**         | 全连接操作                 | `Linear`, `Bilinear`         |
| **归一化层**       | 标准化数据                 | `BatchNorm1d`, `LayerNorm`   |
| **池化层**         | 降维或压缩特征             | `MaxPool2d`, `AvgPool3d`     |
| **激活函数**       | 引入非线性                 | `ReLU`, `Sigmoid`, `Tanh`    |
| **嵌入层**         | 处理离散特征（如词向量）   | `Embedding`, `EmbeddingBag`  |
| **稀疏层**         | 处理稀疏数据               | `Embedding`, `SparseLinear`  |

## 卷积层

### `.Conv2d()`

二维卷积层，自动初始化权重(He初始化)

**参数：**

- in_channels(`int`)：输入信号的通道
- out_channels(`int`)：卷积产生的通道
- kerner_size(`int` or `tuple`)：卷积核的尺寸
- stride(`int` or `tuple`, `optional`)：卷积步长
- padding(`int` or `tuple`, `optional`)：输入的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`)：卷积核元素之间的间距
- groups(`int`, `optional`)：从输入通道到输出通道的阻塞连接数
- bias(`bool`, `optional`)：如果`bias=True`，添加偏置

**变量:**

`weight`：卷积的权重，大小是(`out_channels`, `in_channels`, `kernel_size`)

`bias`：卷积的偏置系数，大小是（`out_channel`）

### `.ConvTranspose2d()`

二维转置卷积操作，有时（但不正确地）称为解卷积操作

**参数：**

- in_channels(`int`)：输入信号的通道数
- out_channels(`int`)：卷积产生的通道数
- kerner_size(`int` or `tuple`)：卷积核的大小
- stride(`int` or `tuple`,`optional`)：卷积步长
- padding(`int` or `tuple`, `optional`)：输入的每一条边补充0的层数
- output_padding(`int` or `tuple`, `optional`)：输出的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`)：卷积核元素之间的间距
- groups(`int`, `optional`)：从输入通道到输出通道的阻塞连接数
- bias(`bool`, `optional`)：如果`bias=True`，添加偏置

### 其他卷积层

- `.Conv1d()`一维卷积层
- `.Conv3d()`三维卷积层
- `.ConvTranspose1d()`
- `.ConvTranspose3d()`

## 池化层

### `.MaxPool2d()`

**参数：**

- kernel_size(`int` or `tuple`)：max pooling的窗口大小
- stride(`int` or `tuple`, `optional`)：max pooling的窗口移动的步长。默认值是`kernel_size`
- padding(`int` or `tuple`, `optional`)：输入的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`)：一个控制窗口中元素步幅的参数
- return_indices：如果等于`True`，会返回输出最大值的序号，对于上采样操作会有帮助
- ceil_mode：如果等于`True`，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作

### `.MaxUnpool2d()`

`.MaxPool2d()`的逆过程

**参数：**

- kernel_size(`int` or `tuple`)：max pooling的窗口大小
- stride(`int` or `tuple`, `optional`)：max pooling的窗口移动的步长。默认值是`kernel_size`
- padding(`int` or `tuple`, `optional`)：输入的每一条边补充0的层数

### `.AvgPool2d()`

**参数：**

- kernel_size(`int` or `tuple`)：池化窗口大小
- stride(`int` or `tuple`, `optional`)：max pooling的窗口移动的步长。默认值是`kernel_size`
- padding(`int` or `tuple`, `optional`)：输入的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`)：一个控制窗口中元素步幅的参数
- ceil_mode：如果等于`True`，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作
- count_include_pad：如果等于`True`，计算平均池化时，将包括`padding`填充的0

### `.FractionalMaxPool2d()`

分数最大池化

**参数：**

- kernel_size(`int` or `tuple`)：最大池化操作时的窗口大小。可以是一个数字（表示`K*K`的窗口），也可以是一个元组（`kh*kw`）
- output_size：输出图像的尺寸。可以使用一个`tuple`指定(oH,oW)，也可以使用一个数字oH指定一个oH*oH的输出。
- output_ratio：将输入图像的大小的百分比指定为输出图片的大小，使用一个范围在(0,1)之间的数字指定
- return_indices：默认值`False`，如果设置为`True`，会返回输出的索引，索引对 `nn.MaxUnpool2d`有用。

### `.LPPool2d()`

幂平均池化

**参数**

- kernel_size: 池化窗口的大小
- stride：池化窗口移动的步长。`kernel_size`是默认值
- ceil_mode: `ceil_mode=True`时，将使用向下取整代替向上取整

### `.AdaptiveMaxPool2d()`

自适应最大池化

**参数：**

- output_size: 输出信号的尺寸,可以用（H,W）表示`H*W`的输出，也可以使用数字`H`表示`H*H`大小的输出
- return_indices: 如果设置为`True`，会返回输出的索引。对 `nn.MaxUnpool2d`有用，默认值是`False`

### `.AdaptiveAvgPool2d()`

自适应平均池化

- output_size: 输出信号的尺寸,可以用(H,W)表示`H*W`的输出，也可以使用耽搁数字H表示H*H大小的输出

### 其他池化层

- `.MaxPool1d()`
- `.MaxPool3d()`
- `.MaxUnpool1d()`
- `.MaxUnpool3d()`
- `.AvgPool1d()`
- `.AvgPool3d()`
- `.AdaptiveMaxPool1d()`
- `.AdaptiveAvgPool1d()`

## 非线性层

- `.ReLU()`：
    - 参数： inplace-选择是否进行覆盖运算
- `.ReLU6()`：${ReLU6}(x) = min(max(0,x), 6)$
    - 参数： inplace-选择是否进行覆盖运算
- `.ELU()`：$f(x) = max(0,x) + min(0, alpha * (e^x：1))$
- `.PReLU()`：$PReLU(x) = max(0,x) + a * min(0,x)$
    - 参数：
        - num_parameters：需要学习的`a`的个数，默认等于1
        - init：`a`的初始值，默认等于0.25

- `.LeakyReLU()`：$f(x) = max(0, x) + {negative_{slope}} * min(0, x)$
    - 参数：
        - negative_slope：控制负斜率的角度，默认等于0.01
        - inplace-选择是否进行覆盖运算

- `.Threshold()`：
    - 参数：
        - threshold：阈值
        - value：输入值小于阈值则会被value代替
        - inplace：选择是否进行覆盖运算

- `.Hardtanh()`：f(x)=+1,if x>1; f(x)=−1,if x<−1; f(x)=x,otherwise，线性区域的范围[-1,1]可以被调整
    - 参数：
        - min_value：线性区域范围最小值
        - max_value：线性区域范围最大值
        - inplace：选择是否进行覆盖运算

- `.Sigmoid()`：f(x)=1/(1+e−x)
- `.Tanh()`：$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
- `.LogSigmoid()`：$LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))$
- `.Softplus()`：$f(x)=\frac{1}{beta}*log(1+e^{(beta*x_i)})$，ReLU函数的平滑逼近，可使得输出值限定为正数，为了保证数值稳定性，线性函数的转换可以使输出大于某个值
    - **参数：**
        - beta：Softplus函数的beta值
        - threshold：阈值

- `.Softshrink()`
    - $f(x) =  \begin{cases}  x：\lambda, & \text{if } x > \lambda, \\ x + \lambda, & \text{if } x < -\lambda, \\ 0, & \text{if } |x| \leq \lambda \quad \end{cases}$，$\lambda$默认0.5
- `.Softsign()`：$f(x)=x/(1+|x|)$
- `.Tanhshrink()`：$Tanhshrink(x)=x-Tanh(x)$
- `.Softmin()`：$f_i(x)=\frac{e^{-x_i-shift}}{\sum^je^{-x_j-shift}},shift=max(x_i)$
- `.Softmax()`：$f_i(x)=\frac{e^{x_i-shift}}{\sum^je^{x_j-shift}},shift=max(x_i)$
- `.LogSoftmax()`：$f_i(x)=log\frac{e^(x_i)}{a}, a=\sum^je(x_j)$

## 归一层

- `.BatchNorm1d()`



## Recurrent层

- `.RNN()`：
- `.LSTM()`：
- `.GRU()`：
- `.RNNCell()`：
- `.LSTMCell()`：
- `.GRUCell()`：



## 线性层

### `.Linear()`

**参数：**

- **in_features**：每个输入样本的大小
- **out_features**：每个输出样本的大小
- **bias**：若设置为False，这层不会学习偏置。默认值：True







# 容器

| 子模块         | 功能描述         | 示例类          |
| -------------- | ---------------- | --------------- |
| **Sequential** | 顺序堆叠层       | `nn.Sequential` |
| **ModuleList** | 动态列表式层管理 | `nn.ModuleList` |
| **ModuleDict** | 字典式层管理     | `nn.ModuleDict` |
| **Parameter**  | 管理可训练参数   | `nn.Parameter`  |

## `.Parameter()`

**`Parameters` 类**，`Variable` 的子类

-  当`Paramenters`赋值给`Module`的属性时会自动被加到 `Module`的参数列表及`parameters()`迭代器中（有时需缓存一些临时`state`）
-  Parameters不能被 `volatile`，无法设置`volatile=True`，且默认`requires_grad=True`，`Variable`默认`requires_grad=False`

## 参数管理

- `.parameters()`​：返回所有**可训练参数**（当前模块和子模块中所有参数的扁平**迭代器**），**不按模块层级区分**，通常是 **​`torch.nn.Parameter`​**​ 类型的张量，包括权重、偏置等，用于**优化器传参**， 只包含模型中需要梯度的参数，不能直接保存/加载模型结构/状态，可对特定子模块单独调用 `.parameters()`​，分模块训练需用 `named_parameters()`​，通过字符串 `name`​ 判断参数属于哪个子模块

- `.named_parameters()`​：返回模型中**所有可训练参数**的名字和参数本身的迭代器

- `.named_children()`​：返回模块的**直接**子模块（带名字）

- `.named_modules()`​：返回模型中**所有**子模块及其名称

  ​    

- `.modules()`：递归地返回所有子模块的**生成器**

    - `._modules`：内部属性，**有序字典**，用于维护子模块的添加顺序，通常不由用户使用
    - `add_module(name, module)`：将子模块添加到模型的 `_modules` 字典，无返回值，`name` (str)子模块的名称，将用于在模型的 `__setattr__` 方法中注册子模块，并作为键存储在模型的 `_modules` 字典中；`module` (`nn.Module`)要添加的子模块

- `.state_dict()`​：获取所有模型的“状态信息”，返回一个 **字典（**​**​`OrderedDict`​**​ **）** ，键是参数或缓存变量的名字，值是张量，包含所有权重和偏置、有些非训练参数（如 BatchNorm 的 running\_mean 和 running\_var），常用于：

- **保存模型参数：**  `torch.save(model.state_dict(), "model.pth")`​

- **加载模型参数：**  `model.load_state_dict(torch.load("model.pth"))`​

| 特性             | `.parameters()`​                 | `.state_dict()`​                         |
| ---------------- | ------------------------------- | --------------------------------------- |
| 返回类型         | 迭代器，元素是`Parameter`​       | 有序字典，key 是名字，value 是张量      |
| 包含内容         | 可训练参数（需要梯度）          | 所有参数 + 缓存（如 BN 层的均值、方差） |
| 用途             | 通常用于传给优化器              | 通常用于保存、加载模型状态              |
| 是否包含名字     | ❌（除非用`named_parameters()`​） | ✅（key 就是参数名）                     |
| 是否包含缓存参数 | ❌                               | ✅（如`running_mean`​,`running_var`​）     |

`nn.Conv2d`有属性`kernel_size`







# 损失函数

| 子模块       | 功能描述   | 示例类                        |
| ------------ | ---------- | ----------------------------- |
| **回归损失** | 连续值损失 | `MSELoss`, `L1Loss`           |
| **分类损失** | 离散值损失 | `CrossEntropyLoss`, `BCELoss` |
| **特殊损失** | 如对抗损失 | `TripletMarginLoss`           |



# 工具类

| 子模块         | 功能描述   | 示例类/函数                               |
| -------------- | ---------- | ----------------------------------------- |
| **初始化工具** | 权重初始化 | `init.xavier_uniform_()`                  |
| **Dropout**    | 防止过拟合 | `Dropout`, `AlphaDropout`                 |
| **数据并行**   | 多GPU训练  | `DataParallel`, `DistributedDataParallel` |
| **量化工具**   | 模型压缩   | `QuantStub`, `DeQuantStub`                |

## `.init`

模块中所有函数均用于初始化神经网络参数，均在`torch.no_grad()`模式下运行，且不会被自动求导所考虑

### `.calculate_gain()`

对于给定的非线性函数，返回推荐的增益值

| nonlinearity    | gain                                     |
| --------------- | ---------------------------------------- |
| linear/constant | 1                                        |
| Conv{1,2,3}d    | 1                                        |
| Sigmoid         | 1                                        |
| Tanh            | $\frac{3}{5}$                            |
| ReLU            | $\sqrt2$                                 |
| Leaky ReLU      | $\sqrt{\frac{2}{1+{negative\_slope}^2}}$ |
| SELU            | $\frac{3}{4}$                            |

**参数：**

- `nonlinearity`：非线性函数名`str`，选项`'linear'`，`'conv1d'`，`'conv2d'`，`'conv3d'`，`'conv_transpose1d'`，`'conv_transpose2d'`，`'conv_transpose3d'`，`'sigmoid'`，`'tanh'`，`'relu'`，`'leaky_relu'`，`'selu'`
- `param`：非线性函数的可选参数

```python
# leaky_relu with negative_slope=0.2
gain = nn.init.calculate_gain('leaky_relu', 0.2)
```

### `.uniform_()`

从均匀分布`U(a, b)`中生成值，填充输入的张量或变量

**参数：**

- `tensor`：n维的torch.Tensor
- `a=0.0`：均匀分布的下界
- `b=1.0`：均匀分布的上界
- `generator=None`：用于采样的torch生成器

### `.normal_()`

从正态分布N(mean, std^2^)中生成值，填充输入的张量或变量

**参数：**

- `tensor`：n维的torch.Tensor
- `mean=0.0`：正态分布的均值
- `std=1.0`：正态分布的标准差
- `generator=None`：用于采样的torch生成器

### `.constant_()`

用`val`的值填充输入的张量或变量

**参数：**

- `tensor`：n维的torch.Tensor
- `val`：用来填充张量的值

### `.ones_()`

用标量值1填充输入张量

**参数：**

- `tensor`：一个n维`torch.Tensor`

### `.zeros_()`

用标量值0填充输入张量

**参数：**

- `tensor`：一个n维`torch.Tensor`

### `.eye_()`

用**单位矩阵**来填充输入张量，在线性层尽可能多的保存输入特性以保持恒等性

**参数：**

- `tensor`：一个n维`torch.Tensor`

### `.dirac_()`

用狄拉克δ函数填充 {3, 4, 5} 维输入Tensor

在卷积层中，尽可能多地保留输入通道以保持其恒等性，如果 groups > 1，则每个通道组保持恒等性

- `tensor`：一个 {3, 4, 5} 维 torch.Tensor
- `groups=1`：卷积层中的组数

### `.xavier_uniform_()`

用`Xavier`均匀分布填充输入的张量，采样自`U(-a, a)`，其中
$$
a=gain \times \sqrt{\frac{6}{fan\_in + fan\_out}}
$$
**参数：**

- `tensor`：n维的torch.Tensor
- `gain`：可选的缩放因子
- `generator=None`：用于采样的torch生成器

### `.xavier_normal_()`

用`Xavier`正态分布填充输入的张量，值采样自N(0, std^2^)，其中
$$
std= gain \times \sqrt{\frac{2}{fan\_in+fan\_out}}
$$
**参数：**

- `tensor`：n维的torch.Tensor
- `gain`：可选的缩放因子
- `generator=None`：用于采样的torch生成器

### `.kaiming_uniform_()`

用`Kaiming`均匀分布填充输入的张量，值采样自`U(-bound, bound)`，其中
$$
bound=gain \times \sqrt{\frac{3}{fan\_mode}}
$$
**参数：**

- `tensor`：n维的torch.Tensor
- `a`：这层之后使用的rectifier整流器的负斜率，仅与 `'leaky_relu'` 一起使用
- `mode`：`'fan_in'`（默认）或 `'fan_out'`， `'fan_in'` 在前向传播中保留权重的方差大小，`'fan_out'` 在反向传播中保留大小
- `nonlinearity`：非线性函数，建议仅与 `'relu'` 或 `'leaky_relu'`（默认）
- `generator=None`：用于采样的torch生成器

### `.kaiming_normal_()`

用`Kaiming`正态分布填充输入的张量，值采样自N(0, std^2^)，其中
$$
std=\frac{gain}{\sqrt{fan\_mode}}
$$
**参数：**

- `tensor`：一个 n 维 torch.Tensor
- `a`：此层后使用的整流器的负斜率，仅与 `'leaky_relu'` 一起使用
- `mode`：`'fan_in'`（默认）或 `'fan_out'`， `'fan_in'` 在前向传播中保留权重的方差大小，`'fan_out'` 在反向传播中保留大小
- `nonlinearity`：非线性函数，建议仅与 `'relu'` 或 `'leaky_relu'`（默认）
- `generator=None`：用于采样的torch生成器

### `.trunc_normal_()`

用截断正态分布中抽取的值填充输入张量，值从N(mean, std^2^)抽取，超出`(a, b)`则**重新抽取**

**参数：**

- `tensor`：一个 n 维 torch.Tensor
- `mean`：正态分布的均值
- `std`：正态分布的标准差
- `a`：最小截断值
- `b`：最大截断值
- `generator=None`：用于采样的torch生成器

### `.orthogonal_()`

用（半）正交矩阵填充输入的张量，输入**至少2维**，超出的维度会被展平，非零元素生成自N(0, std^2^)

**参数：**

- `tensor`：n维的torch.Tensor，其中`n >= 2`
- `gain`：可选的缩放因子
- `generator=None`：用于采样的torch生成器

### `.sparse_()`

将2维输入填充为稀疏矩阵，非零元素从N(0, std^2^)抽取

**参数：**

- `tensor`：n维的torch.Tensor
- `sparsity`：每列中需要被设置成零的元素比例
- `std`：用于生成非零值的正态分布的标准差
- `generator=None`：用于采样的torch生成器

## `Dropout`

随机置零输入张量的部分元素，测试时自动关闭，防止过拟合，防止神经元协同适应

| 类型                  | 特点                                     | 数学形式            |
| :-------------------- | :--------------------------------------- | :------------------ |
| `Dropout`             | 标准丢弃（训练时按概率置零，测试时缩放） | *y*=1−*p**x*⋅*m*    |
| `Dropout1d`           | 对通道维度整体丢弃（1D特征图）           | 按通道置零          |
| `Dropout2d`           | 对通道维度整体丢弃（图像数据）           | 按通道置零          |
| `Dropout3d`           | 对通道维度整体丢弃（图像数据）           | 按通道置零          |
| `AlphaDropout`        | 保持输入均值和方差（适用于 SELU 激活）   | *y*=(*α**m*+*β*)*x* |
| `FeatureAlphaDropout` |                                          |                     |

### `.Dropout()`

**训练时**以概率 `p` 随机将输入张量的一些**元素**归零，输出按因子$\frac{1}{1-p}$进行缩放，每次前向调用/每个通道**独立选择**，并从**伯努利分布**中采样

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

### `.Dropout1d()`

随机将整个通道置零，一个通道是1D特征图，通常输入来自 `nn.Conv1d` 模块

若特征图中的相邻像素强相关，`.Dropout()`不会有效正则化，只会导致学习效率降低，此时`.Dropout1d()`可促进特征图之间的独立性

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

### `.Dropout2d()`

随机将整个通道置零，一个通道是一个二维特征图，通常输入来自 `nn.Conv2d` 模块

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

:warning: 对3D 输入执行`.Dropout1d()`（历史原因）

### `.Dropout3d()`

随机将整个通道置零，一个通道是 3D 特征图，通常输入来自 `nn.Conv3d` 模块

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

### `.AlphaDropout()`

一种可保持**自归一化**的 Dropout，对于均值为零、标准差为一的输入，输出会**保持输入的原始均值和标准差**，与 SELU 激活函数配合使用，后者可确保输出具有零均值和单位标准差

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

### `.FeatureAlphaDropout()`

随机屏蔽整个通道，通道是特征图，通常输入来自 `nn.AlphaDropout` 模块

与常规 Dropout 将激活设置为零不同，此处的激活被设置为 SELU 激活函数的负饱和值

**参数：**

- `p=0.5`：元素被归零的概率
- `inplace=False`：`True`原地执行

## 数据并行

实现多 GPU 并行训练，加速模型训练

| 方法                              | 通信方式     | 适用场景                                                     | 代码示例                         |
| :-------------------------------- | :----------- | :----------------------------------------------------------- | :------------------------------- |
| `.DataParallel`（DP）             | 单进程多线程 | 单机多卡（小规模），在模块级数据并行                         | `model = nn.DataParallel(model)` |
| `.DistributedDataParallel`（DDP） | 多进程       | 多机多卡/大规模训练，基于 `torch.distributed` 的模块级分布式数据并行 | 需配合 `torch.distributed`使用   |

```python
import torch.distributed as dist

# 初始化进程组
dist.init_process_group(backend='nccl')		# 用GPU时，nccl后端是目前最快且高度推荐的后端，这适用于单节点和多节点分布式训练

# 包装模型
model = nn.parallel.DistributedDataParallel(
    model,
    device_ids=[local_rank],
    output_device=local_rank
)
```

### `.DataParallel()`

在模块级别实现数据并行，将输入沿批处理维度（其他对象将为每个设备复制一次）进行分块，从而在指定设备上并行化给定 `module`，

前向传播中，模块在每个设备上复制，每个副本处理输入的一部分；反向传播中，来自每个副本的梯度会被求和到原始模块中

**注意：**

- 批处理大小应大于所使用的 GPU 数量
- 即使只有一个节点，也建议使用`DistributedDataParallel`进行多 GPU 训练
- 张量将按指定维度（默认为 0）进行**分散**；元组、列表和字典类型将进行浅拷贝；其他类型将在不同线程之间共享
- 并行的 `module` 在运行此`DataParallel`模块之前，必须将其参数和缓冲区放在 `device_ids[0]` 上
- `module`的`forward()`返回标量时，将返回一个向量，包含来自每个设备的结果
- 每次正向传播中`module`会在每个设备上**复制**，因此在 `forward` 中对运行模块的任何更新都将丢失； `device[0]`上的副本将与`module`共享其参数和缓冲区的存储，因此对`device[0]`上的参数或缓冲区的**原地**更新将被记录，`BatchNorm2d`和`spectral_norm()`依赖此行为来更新缓冲区

**参数：**

- `module`：要并行的模块
- `device_ids`：CUDA 设备，默认所有设备
- `output_device`：输出设备的所在地，默认`device_ids[0]`
- `dim=0`

### `.parallel.DistributedDataParallel()`

基于 `torch.distributed` 的模块级别分布式数据并行，在每个模型副本之间同步梯度来实现数据并行

**注意：**

- 在单节点多 GPU 数据并行训练中比 `torch.nn.DataParallel` 快得多
- 不会对参与的 GPU 进行分块或分片，用户负责定义如何进行分片
- 在`N`个GPU上使用时应启动`N`个进程，确保每个进程专门处理 0 到 N-1 的单个 GPU
- 参数永远不会在进程之间广播。该模块会对梯度执行 all-reduce 操作，并假设它们将在所有进程中以相同方式被优化器修改；缓冲区（如 BatchNorm 统计信息）在每次迭代中从 rank 0 的模块广播到系统中的所有其他副本

**参数：**

- `module`：要并行化的模块
- `device_ids`：CUDA 设备，多设备模块和 CPU 模块时必须为 `None`

#### `.join()`

DDP 中训练不等量输入跨进程的**上下文管理器**，跟踪已加入的 DDP 进程，并通过插入集体通信操作来“模拟”前向和后向传播，以匹配未加入的 DDP 进程创建的通信操作，这将确保每个集体调用都有一个已加入的 DDP 进程对应的调用，从而防止在跨进程训练不等量输入时发生的挂起或错误

一旦所有 DDP 进程都加入，上下文管理器会将最后一个加入进程的模型广播到所有进程，以确保模型在所有进程之间是相同的

#### `.join_hook()`

通过在正向和反向传播中**镜像通信**来支持不等量输入的训练

#### `.no_sync()`

禁用 DDP 进程之间梯度同步的上下文管理器，在此上下文内，梯度将在模块变量上累积，这些变量稍后将在退出上下文的第一个前向-后向传递中同步

#### `.register_comm_hook()`

注册用于用户定义的 DDP 跨多个工作节点梯度聚合的通信 hook

## 量化工具

量化：以低于浮点精度的比特宽度执行计算和存储张量的技术，减少模型存储大小和计算开销，PyTorch 支持逐张量和逐通道非对称线性量化

## `torch.nn.quantized`& `torch.quantization`)

### 核心模块

| 模块                               | 作用                   |
| :--------------------------------- | :--------------------- |
| `QuantStub`                        | 标记输入量化位置       |
| `DeQuantStub`                      | 标记输出反量化位置     |
| `torch.quantization.observer`      | 统计张量范围（校准）   |
| `torch.quantization.fake_quantize` | 模拟量化效果（训练时） |



------



# 特殊功能

| 子模块              | 功能描述         | 示例类                              |
| ------------------- | ---------------- | ----------------------------------- |
| **Transformer**     | 自注意力模型     | `Transformer`, `TransformerEncoder` |
| **Vision Modules**  | 计算机视觉专用层 | `PixelShuffle`, `Upsample`          |
| **Adaptive Layers** | 动态调整输出尺寸 | `AdaptiveAvgPool2d`                 |




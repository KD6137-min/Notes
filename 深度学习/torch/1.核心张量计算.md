`torch.Tensor`，包含**单一数据类型**的多维矩阵

# 创造与转换

- `torch.tensor()`：创建新的独立张量，自动推断数据类型（或通过dtype指定）
- `torch.empty()`：创建一个未初始化的张量，值为内存中残留数据，不可预测，当马上被写入时使用，减少不必要的初始化
- `torch.from_numpy()`：将ndarray转为tensor，**共享内存**
    - 该方式下tensor将严格继承numpy数据类型（tensor不兼容numpy的float64，需手动调整类型）

> :o: 注意区分：
>
> - `tensor.numpy()`：将张量转换为numpy数组，**共享内存**
> - `DataFrame.to_numpy()`：Pandas方法，不共享内存，可通过copy参数控制，可避免object类型，推荐
>
> :o: 特别注意：
>
> ​	numpy数组中的`bool`列会被推断为`object`类型，pandas的`.values()`方法可能返回`object`数组，而object类型无法转换为tensor类型，需将其转换为`float32`或`int64`，优先用`.to_numpy(dtype=...)`替换`.values()`
>
> 典型步骤：
>
> ```python
> # 假设 all_features 是包含混合类型的 DataFrame
> # 1. 转换非数值列
> object_cols = all_features.select_dtypes(include=['object']).columns
> for col in object_cols:
>     all_features[col] = pd.to_numeric(all_features[col], errors='coerce')
> 
> # 2. 填充缺失值
> all_features.fillna(0, inplace=True)
> 
> # 3. 确保全为 float32
> all_features = all_features.astype(np.float32)
> 
> # 4. 转换为张量
> n_train = train_data.shape[0]
> train_features = torch.tensor(
>     all_features[:n_train].to_numpy(), 
>     dtype=torch.float32
> )
> ```
>

# 底层数据结构

`Storage`类，用于实际存储张量

张量可看作对`Storage`的视图，通过定义偏移量、步长、形状等元信息实现对Storage中数据的不同维度访问

特点：

- 连续内存块，无维度信息
- 可被多张量共享，节省内存
- 每个Storage对应一种数据类型，不可动态修改

# 类型

七种CPU tensor类型和八种GPU tensor类型：

| Data tyoe                  | CPU tensor           | GPU tensor                |
| -------------------------- | -------------------- | ------------------------- |
| 32-bit floating point，默认类型 | `torch.FloatTensor`  | `torch.cuda.FloatTensor`  |
| 64-bit floating point      | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16-bit floating point      | N/A                  | `torch.cuda.HalfTensor`   |
| 8-bit integer (unsigned)   | `torch.ByteTensor`   | `torch.cuda.ByteTensor`   |
| 8-bit integer (signed)     | `torch.CharTensor`   | `torch.cuda.CharTensor`   |
| 16-bit integer (signed)    | `torch.ShortTensor`  | `torch.cuda.ShortTensor`  |
| 32-bit integer (signed)    | `torch.IntTensor`    | `torch.cuda.IntTensor`    |
| 64-bit integer (signed)    | `torch.LongTensor`   | `torch.cuda.LongTensor`   |

类型转换：`tensor.type(dtype)`方法（区分于`type()`内置函数）

比较技巧：`cmp = y_hat.type(y.dtype) == y`，转换为相同类型后比较

# 判断

| `torch.`                   |                                                              |
| -------------------------- | ------------------------------------------------------------ |
| `is_tensor(obj)`           | obj 是 tensor则True                                          |
| `is_storage(obj)`          | obj 是 storage对象则True                                     |
| `is_complex(input)`        | `input`是复数数据类型(`torch.complex64`或`torch.complex128`)则True |
| `is_conj(input)`           | `input` 是共轭张量(共轭位设置为 True)则True                  |
| `is_floating_point(input)` | `input`是浮点数据类型(`torch.float64`、`torch.float32`、`torch.float16`、`torch.bfloat16` )则True |
| `is_nonzero(input)`        | `input` 是单元素张量且在类型转换后不等于零则True             |

# 基础设置

| `torch.`                    |                                                  |
| --------------------------- | ------------------------------------------------ |
| `set_default_dtype()`       | 将默认浮点 dtype 设置为 `d`                      |
| `get_default_dtype()`       | 获取当前默认浮点 `torch.dtype`                   |
| `set_default_device()`      | 设置默认 `torch.Tensor` 在 `device` 上分配       |
| `get_default_device()`      | 获取默认 `torch.Tensor` 在 `device` 上分配       |
| `set_default_tensor_type()` | 设置默认张量类型，之后创建的张量默认使用指定类型 |
| `set_printoptions()`        | 设置打印选项                                     |
| `set_flush_denormal()`      | 禁用 CPU 上的非正常浮点数                        |

# 与相似类型对比

- **PIL图像对象**与tensor：

    - 张量：通道优先`[C, H, W]`，数值范围通常为`[0, 1]`或标准化后的浮点数，`matplotlib`的`imshow()`无法直接识别其数据结构，需转为NumPy数组（`[H, W, C]`）并自动映射到`[0, 255]`整数范围
    - PIL图像通道最后`[H, W, C]`，像素值（范围`[0, 255]`），与`imshow()`的输入格式完全兼容

    ```python
    if torch.is_tensor(img):
        ax.imshow(img.numpy())
    else:
        ax.imshow(img)
    ```

- **ndarray**与tensor：

    - ndarray聚焦数值操作，**数据存储与解释方式分离**，底层分**数据缓冲区（Data Buffer）**和**元数据（Metadata）**，**仅限CPU计算**，无法直接访问GPU，**无自动微分能力**，无法记录运算历史，依赖连续内存与矢量化
    - tensor在ndarray基础上扩展，核心结构为`Storage + 梯度计算上下文`，支持异构计算、设备感知、GPU加速、分布式、操作符重载、异步执行、内核融合，**深度集成自动微分**：
        - **计算图构建**：每次运算生成`Function`节点，记录输入/输出依赖（如`AddBackward`）
        - **梯度传播**：调用`.backward()`时，沿`grad_fn`链反向计算梯度，并存储于`.grad`属性中

# 形状与内存布局

张量**底层内存布局方式（tensor layout）：**即张量内部的数据在内存中怎么存放

| Layout 类型        | 描述                                   | 用途/例子                          |
| ------------------ | -------------------------------------- | ---------------------------------- |
| `torch.strided`​    | 默认布局，行优先，支持切片、转置、广播 | 常规张量计算，比如卷积、线性层等   |
| `torch.sparse_coo`​ | 稀疏张量，压缩存储稀疏数据             | 大量 0 的稀疏矩阵（加速节省内存）  |
| `torch.sparse_csr`​ | 压缩行存储的稀疏格式                   | 更适合稀疏矩阵乘法（如图神经网络） |
| `torch._mkldnn`​    | 针对 MKL-DNN 优化的特殊格式            | 用于 Intel CPU 的加速              |

**分类：**

- **连续张量**：张量在内存中为**连续内存**区域**连续排布**，非连续张量只是索引方式变了
    - **​`tensor.is_contiguous()`​** ​：判断是否为连续张量
    - **​`tensor.contiguous()`​** ​：把非连续张量转为连续张量
- **非连续张量**：对张量做了某些操作：**转置（**​ **​`.t()`​** ​ **）** 、**切片**、**permute / transpose**，只改变了张量的“视图”（即访问方式），而没有真正移动内存中的数据

**形状操作：**

- `.shape`属性或`.size()`方法：返回`torch.Size`类型的形状，
    - 使用方式不同：`.size(0)`取第0维，`.shape[0]`取第0维（Numpy中没有`.size()`）

- `.view()`：返回原张量视图（原张量必须是连续的），不创建新张量，速度快
- `.reshape()`：自动处理非连续张量（自动copy），可能返回新张量
    - `torch.reshape(tensor, shape)`和`tensor.reshape(shape)`：完全等价
    - `.reshape(-1)`或`.reshape(1, -1)`​或`.reshape((-1,))`：将张量展平成一维向量

> :o: 注意：
>
> ​	形状`(n,)`，即一维张量，有n个元素，无第二个维度

# 原地操作

torch方法末尾的`_`​表原地操作

推荐写法：

```python
with torch.no_grad():
    weight -= something
```

- `+=` / `-=`：**优先**调用`INPLACE_ADD`​（原地加法）操作符，最终是否原地修改不确定
    - **可变对象**（如列表、字典、集合）：直接修改原对象，无需创建新对象
    - **不可变对象**（如整数、字符串、元组）`INPLACE_ADD`​实际会创建新对象并重新赋值给变量
- `[:]`：完整切片，确保原地修改

## `.copy_()`

把参数拷贝到该tensor，就地修改，官方推荐做法，自动对齐dtype，自动迁移device，保证shape匹配

# 乘法

- `torch.matmul()`：通用矩阵乘法，推荐，支持广播，灵活
- `torch.mm()`：**局限**于二维矩阵乘法，不支持广播和批量操作
- `torch.mv()`：**局限**于矩阵与向量乘法，不支持广播和批量操作
- `torch.mul()`：**元素级**乘法，对应位置的元素相乘

# 分割

## `.split()`

自定义分割，非均等，**返回视图**，更灵活，需手动确保分割参数与维度长度匹配

支持两种形式的参数：整数（每个子张量的固定大小）、列表（自定义每个子张量的大小）

## `.chunk()`

均等分割，**返回视图**

仅支持整数（分割的子张量数量），自动计算每块大小，无法整除时最后一块略小

更快

 ## `.unbind()`

沿指定维度dim分解张量，返回一个元组，元组中的每个元素是原张量在该维度上的切片

```python
torch.unbind(input, dim=0) → Tuple[Tensor]
```

- 维度变化：子张量的维度数比原张量少 1（被移除的维度消失）
- 子张量数量：等于原张量在  dim  维度上的长度

# 常用操作

## `.nonzero()`

你是不是想问 **`torch.nonzero`** 呢？🙂
在 PyTorch 里没有 `torch.nonzeor`，正确的函数是 **`torch.nonzero`**。

------

### 📌 `torch.nonzero` 用法

`torch.nonzero(input, as_tuple=False)`
它的作用是：

- 返回输入张量中 **非零元素的位置**。

#### 示例 1：基础用法

```python
import torch

x = torch.tensor([[0, 1, 0],
                  [2, 0, 3]])

idx = torch.nonzero(x)
print(idx)
```

输出：

```
tensor([[0, 1],
        [1, 0],
        [1, 2]])
```

说明非零元素分别在 `(0,1), (1,0), (1,2)`。

------

#### 示例 2：返回坐标元组

如果 `as_tuple=True`，返回的是每一维的索引张量：

```python
rows, cols = torch.nonzero(x, as_tuple=True)
print(rows)  # tensor([0, 1, 1])
print(cols)  # tensor([1, 0, 2])
```

这样更方便用来做索引：

```python
print(x[rows, cols])  # tensor([1, 2, 3])
```

------

⚠️ 注意：

- 如果你只想要 **第一个非零元素的索引**，可以再加 `.squeeze()[0]`。
- 如果要判断是否有非零元素，可以用 `torch.any(x)`。

------

要不要我帮你对比一下 **`torch.nonzero`** 和 **`numpy.nonzero`** 的差异？

## `.max()`

对的 👍，在 PyTorch 里 **`torch.max`** 的行为要分两种情况：

------

### ① 直接对张量调用

```python
x = torch.tensor([1, 3, 2])
m = torch.max(x)
print(m)  # tensor(3)
```

👉 只返回一个值：**最大值**。

------

### ② 指定 `dim`

如果你写成：

```python
values, indices = torch.max(x, dim=0)
```

就会返回 **两个值**：

1. **values**：最大值
2. **indices**：最大值所在的位置（索引）

------

#### 示例 1：一维向量

```python
x = torch.tensor([1, 3, 2])
values, indices = torch.max(x, dim=0)

print(values)   # tensor(3)
print(indices)  # tensor(1)   (最大值在下标1)
```

------

#### 示例 2：二维张量

```python
x = torch.tensor([[1, 5, 2],
                  [3, 4, 7]])

values, indices = torch.max(x, dim=1)  # 按行取最大值
print(values)   # tensor([5, 7])
print(indices)  # tensor([1, 2])
```

- 第 0 行最大值是 5（索引 1）
- 第 1 行最大值是 7（索引 2）

------

✅ **总结**

- `torch.max(x)` → 只返回一个标量（最大值）。
- `torch.max(x, dim=d)` → 返回 **(values, indices)**。

------

要不要我顺便帮你整理一下 `torch.min`、`torch.argmax`、`torch.argmin` 和 `torch.max` 的对比表？这样查起来一目了然。

## `.norm()`

计算张量的范数，支持多种范数，默认为𝐿2范数

```python
tensor.norm(p='fro', dim=None, keepdim=False, out=None)
```

**​`p`​**：范数的类型

- `p=2`​：L2范数，默认值，计算欧几里得范数
- `p=1`​：L1范数，计算元素的绝对值之和
- `p=float('inf')`​：L∞范数，计算最大绝对值元素
- `p='fro'`​：Frobenius 范数（适用于矩阵）

## `.clamp()`

裁剪，限制张量的数值在指定范围内

```python
torch.clamp(input, min=None, max=None, *, out=None)
# input: 要裁剪的张量
# min: 最小值，低于min则设为min
# max: 最大值，高于max则设为max
# out: 可选，指定输出张量
```

## `.where()`

| 函数                      | 类型    | 功能简介                                | 行为区别                                |
| ------------------------- | ------- | --------------------------------------- | --------------------------------------- |
| `torch.where(cond, A, B)`​ | PyTorch | **条件选择**：cond为True选A，否则选B    | 和NumPy一致，**必须**提供 A、B          |
| `np.where(cond, A, B)`​    | NumPy   | **条件选择**：cond为True选A，否则选B    | A、B 可选：不传A、B时返回索引           |
| `df.where(cond)`​          | Pandas  | 对DataFrame中cond为False的地方替换为NaN | 和上面两个不同，是“保留True，替换False” |

## `.repeat(*sizes)`

将张量在特定维度上重复，返回新张量

- sizes整数元组，与张量形状相同，指定每个维度的重复次数，维度不匹配时
- 不是元素复制，返回的新张量用索引方式复制（类似视图），不占额外空间

## `.gather()`

指定维度和索引，将张量中对应位置的元素提取出来形成新的张量

```python
torch.gather(input, dim, index, out=None) → Tensor
```


- `dim`：指定沿哪个维度进行收集
- `index`：索引张量，其形状需与  input  在非  dim  维度上保持一致，且索引值需在  input  对应维度的范围内

与`torch.take_along_dim`的区别：两者功能类似，但`take_along_dim`允许索引张量形状更灵活，而`gather`对索引形状要求更严格

## `.meshgrid()`

将输入的一维张量扩展为多维网格，生成所有可能的坐标组合

生成所有像素位置的中心坐标网络，用于后续以每个像素为中心生成锚框

```python
torch.meshgrid(*tensors, indexing='ij') → Tuple[Tensor, ...]
```


-  indexing ：指定网格索引模式，可选  'ij' （矩阵索引）或  'xy' （笛卡尔坐标），默认为  'ij' 

    -  数学坐标系（ `indexing='ij'`）：以左上角为原点，网格按行优先排列
    -  图像坐标系（ `indexing='xy'`）：以左下角为原点，适合 OpenCV 或 Matplotlib 的坐标系统
-  返回值：由网格坐标张量组成的元组，每个张量的形状为  (S_0, S_1, ..., S_{N-1}) ，其中  S_i  是输入张量的长度

## `.masked_select()`

根据复杂条件筛选张量中的值

```python
torch.masked_select(input, mask, out=None) → Tensor  
```


-  mask : 布尔类型的掩码张量，与  input  形状相同

- 返回一个将所有被选中的元素平铺后的**一维张量**

输出元素的顺序与输入张量的内存布局相关（行优先）

## `.Generator`类

随机数生成器类，用于管理伪随机数生成算法的状态

 ```python
 g = torch.Generator(device='cuda')	# 创建指定设备的生成器，默认cpu
 ```

可创建多个独立实例

**主要方法与属性：**

- `g.manual_seed(seed)`：设置确定性种子，返回生成器，种子必须非负
- `g.initial_seed()`：获取当前生成器的初始种子值
- `g.get_state()`：返回生成器状态的`ByteTensor`，用于后续恢复状态
- `g.set_state(state)`：将生成器状态恢复为指定`ByteTensor`
- `g.seed()`：生成不确定的**随机种子**（基于系统熵或时间）并设置到生成器
- `g.device`：返回生成器所在的设备

 PyTorch 默认使用全局生成器 `torch.default_generator`，可通过`torch.manual_seed()`控制其种子

| 特性     | torch.Generator实例                      | torch.default_generator全局默认生成器 |
| -------- | ---------------------------------------- | ------------------------------------- |
| 独立性   | 可创建多个独立实例，隔离状态             | 单一实例                              |
| 适用场景 | 需精确控制随机状态的场景（如分布式训练） | 简单快速随机操作                      |
| 线程安全 | 需手动管理多线程中的实例分配             | 非线程安全，需谨慎使用                |




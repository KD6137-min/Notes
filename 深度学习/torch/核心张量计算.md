# 张量

`torch.Tensor`，包含**单一数据类型**的多维矩阵

## 类型

七种CPU tensor类型和八种GPU tensor类型：

| Data tyoe                       | CPU tensor           | GPU tensor                |
| ------------------------------- | -------------------- | ------------------------- |
| 32-bit floating point，默认类型 | `torch.FloatTensor`  | `torch.cuda.FloatTensor`  |
| 64-bit floating point           | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16-bit floating point           | N/A                  | `torch.cuda.HalfTensor`   |
| 8-bit integer (unsigned)        | `torch.ByteTensor`   | `torch.cuda.ByteTensor`   |
| 8-bit integer (signed)          | `torch.CharTensor`   | `torch.cuda.CharTensor`   |
| 16-bit integer (signed)         | `torch.ShortTensor`  | `torch.cuda.ShortTensor`  |
| 32-bit integer (signed)         | `torch.IntTensor`    | `torch.cuda.IntTensor`    |
| 64-bit integer (signed)         | `torch.LongTensor`   | `torch.cuda.LongTensor`   |

## 判断

| `torch.`                   |                                                              |
| -------------------------- | ------------------------------------------------------------ |
| `is_tensor(obj)`           | obj 是 tensor则True                                          |
| `is_storage(obj)`          | obj 是 storage对象则True                                     |
| `is_complex(input)`        | `input`是复数数据类型(`torch.complex64`或`torch.complex128`)则True |
| `is_conj(input)`           | `input` 是共轭张量(共轭位设置为 True)则True                  |
| `is_floating_point(input)` | `input`是浮点数据类型(`torch.float64`、`torch.float32`、`torch.float16`、`torch.bfloat16` )则True |
| `is_nonzero(input)`        | `input` 是单元素张量且在类型转换后不等于零则True             |

## 基础设置

| `torch.`                                                     |                                            |
| ------------------------------------------------------------ | ------------------------------------------ |
| `set_default_dtype()`                                        | 将默认浮点 dtype 设置为 `d`                |
| [`get_default_dtype`](https://docs.pytorch.ac.cn/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype) | 获取当前默认浮点 `torch.dtype`             |
| [`set_default_device`](https://docs.pytorch.ac.cn/docs/stable/generated/torch.set_default_device.html#torch.set_default_device) | 设置默认 `torch.Tensor` 在 `device` 上分配 |
| [`get_default_device`](https://docs.pytorch.ac.cn/docs/stable/generated/torch.get_default_device.html#torch.get_default_device) | 获取默认 `torch.Tensor` 在 `device` 上分配 |
| [`set_default_tensor_type`](https://docs.pytorch.ac.cn/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type) |                                            |
| [`numel`](https://docs.pytorch.ac.cn/docs/stable/generated/torch.numel.html#torch.numel) | 返回 `input` 张量中的元素总数              |
| `set_printoptions`                                           | 设置打印选项                               |
| `set_flush_denormal`                                         | 禁用 CPU 上的非正常浮点数                  |

## 创造与转换

类型转换：`tensor.type(dtype)`​方法（不是`type()`​内置函数）

比较技巧：

```python
# 将y_hat转为和y同类型后比较，否则数据相同类型不同仍为False
cmp = y_hat.type(y.dtype) == y
```

`.numpy()`和`.to_numpy()`区别：

- .numpy()属于PyTorch的`Tensor`​对象方法，共享内存
- .to\_numpy()属于Pandas的`DataFrame`​或`Series`​对象方法，默认不共享内存，可通过copy参数控制，且可避免object类型，推荐（替换`.values()`）

两个核心类型：PIL图像对象和PyTorch张量

- PyTorch张量：通道优先`[C, H, W]`，数值范围通常为`[0, 1]`或标准化后的浮点数，`matplotlib`的`imshow()`无法直接识别其数据结构，需转为NumPy数组（`[H, W, C]`）并自动映射到`[0, 255]`整数范围
- PIL图像：通道最后`[H, W, C]`排列像素值（范围`[0, 255]`），与`imshow()`的输入格式完全兼容

```python
if torch.is_tensor(img):
    ax.imshow(img.numpy())
else:
    ax.imshow(img)
```

`ndarray`与`Tensor`区别：

- **ndarray**

    - 核心设计：​**​数据存储与解释方式分离​**​，底层分两部分：

        - **数据缓冲区（Data Buffer）**：连续内存块，存储同类型元素（如`int32`、`float64`），确保内存紧凑高效

        - **元数据（Metadata）**：包含`dtype`（数据类型）、`shape`（维度形状）、`strides`（跨步字节数）等，用于解释数据布局

    - **仅限CPU计算**，内存分配由Python内存管理器控制，无法直接访问GPU。数据转换（如`np.array()`）需显式复制内存

    - **无自动微分能力**，仅用于数值计算，无法记录运算历史
    - **聚焦数值操作**：API设计围绕矩阵运算（如`reshape`、`transpose`），通过`strides`实现视图（view）而非复制数据（如切片返回原数据引用）
    - 依赖连续内存与矢量化：
        - 内存连续性（C-order/F-order）影响`np.dot()`等函数的缓存效率
        - 通过`np.vectorize()`实现无循环批处理，但底层仍依赖CPU指令

- **Tensor**

    - 在`ndarray`基础上扩展，核心结构：
        - **数据存储（Storage）**：类似`ndarray`的连续内存块，但支持跨设备（CPU/GPU）存储
        - **梯度计算上下文**：包含`grad_fn`（反向传播函数）、`requires_grad`（梯度标记）等，支持自动微分
    - 支持异构计算，通过`.cuda()`将数据移至GPU，底层调用CUDA API分配显存，关键特性：
        - **内存共享**：`torch.from_numpy(np_array)`与原始`ndarray`共享内存，修改任一对象会同步变更（零拷贝）
        - **设备感知**：`device`属性标识数据位置（CPU/GPU），运算自动选择匹配设备
    - 深度集成自动微分：
        - **计算图构建**：每次运算生成`Function`节点，记录输入/输出依赖（如`AddBackward`）
        - **梯度传播**：调用`.backward()`时，沿`grad_fn`链反向计算梯度，并存储于`.grad`属性中
    - 深度优化与扩展：
        - **GPU加速运算**：如`torch.matmul()`调用CuBLAS库实现高效矩阵乘法
        - **分布式支持**：`DistributedTensor`支持跨设备分片存储（如模型并行）
        - **操作符重载**：`+`、`*`等运算符重载为`torch.add()`、`torch.mul()`，简化代码
    - 多层次加速：
        - **内核融合（Kernel Fusion）**：将多个操作（如`conv2d + relu`）合并为单一GPU内核，减少显存读写
        - **异步执行**：CUDA流（Stream）并行执行计算与数据迁移，提升吞吐量

|              | **ndarray**                  | **Tensor**                          |
| ------------ | ---------------------------- | ----------------------------------- |
| **内存结构** | 数据+元数据分离，连续CPU内存 | 扩展存储结构，支持跨设备+梯度上下文 |
| **计算设备** | 仅CPU                        | CPU/GPU无缝切换，支持内存共享       |
| **自动微分** | 不支持                       | 内置计算图与梯度传播机制            |
| **API设计**  | 数值计算基础操作             | 深度优化运算+分布式扩展             |
| **性能优化** | 内存布局优化+CPU矢量化       | 内核融合+异步执行+GPU加速           |

:warning:  numpy数组中的`bool`列会被推断为`object`而无法转换为tensor类型，需将其转换为`float32`或`int64`

pandas的`.values()`可能返回object数组，优先用`.to_numpy(dtype=...)`

```python
array = all_features.to_numpy(dtype=np.float32)
```

典型步骤：

```python
# 假设 all_features 是包含混合类型的 DataFrame
# 1. 转换非数值列
object_cols = all_features.select_dtypes(include=['object']).columns
for col in object_cols:
    all_features[col] = pd.to_numeric(all_features[col], errors='coerce')

# 2. 填充缺失值
all_features.fillna(0, inplace=True)

# 3. 确保全为 float32
all_features = all_features.astype(np.float32)

# 4. 转换为张量
n_train = train_data.shape[0]
train_features = torch.tensor(
    all_features[:n_train].to_numpy(), 
    dtype=torch.float32
)
```

## 形状与内存布局

**连续张量**：张量在内存中为**连续内存**区域**连续排布**，非连续张量只是索引方式变了

**非连续张量**：对张量做了某些操作：**转置（**​ **​`.t()`​** ​ **）** 、**切片**、**permute / transpose**，只改变了张量的“视图”（即访问方式），而没有真正移动内存中的数据

- **​`tensor.is_contiguous()`​** ​：判断是否为连续张量
- **​`tensor.contiguous()`​** ​：把非连续张量转为连续张量

### `.view()`​和`.reshape()`​区别

| 特性             | `view()`​                         | `reshape()`​                         |
| ---------------- | -------------------------------- | ----------------------------------- |
| 是否需要连续内存 | ✅ 必须是连续的张量（contiguous） | ✅ 会自动处理非连续张量（自动 copy） |
| 是否创建新张量   | ❌ 返回的是原张量的视图（view）   | ✅ 可能返回新张量（复制数据）        |
| 错误处理         | ⚠ 非连续张量会报错               | ✅ 自动兼容                          |
| 效率             | ✅ 更快                           | ⭕ 稍慢（因为可能涉及复制）          |

张量**底层内存布局方式（tensor layout）：即张量内部的数据在内存中怎么存放**

| Layout 类型        | 描述                                   | 用途/例子                          |
| ------------------ | -------------------------------------- | ---------------------------------- |
| `torch.strided`​    | 默认布局，行优先，支持切片、转置、广播 | 常规张量计算，比如卷积、线性层等   |
| `torch.sparse_coo`​ | 稀疏张量，压缩存储稀疏数据             | 大量 0 的稀疏矩阵（加速节省内存）  |
| `torch.sparse_csr`​ | 压缩行存储的稀疏格式                   | 更适合稀疏矩阵乘法（如图神经网络） |
| `torch._mkldnn`​    | 针对 MKL-DNN 优化的特殊格式            | 用于 Intel CPU 的加速              |

```python
# torch.strided，内存为连续的一维数组[1, 2, 3, 4]，通过 strides(步长)决定怎么“跳”到下一个元素
a = torch.tensor([[1, 2], [3, 4]])
# a.stride()可能返回 (2, 1)，意思是：行之间隔 2 个元素，列之间隔 1 个元素
```

`reshape(-1)`或`reshape(1, -1)`​：将张量展平成一维向量

`torch.reshape(tensor, shape)`​和`tensor.reshape(shape)`​：完全等价

(n, )形状

一维张量，有n个元素，无第二个维度

## 

## 数学运算与数值操作

**原地操作（in-place）** ：

- torch中方法末尾的`_`​表原地操作
- +\=/-=：**优先**调用`INPLACE_ADD`​（原地加法）操作符，对**可变对象**（如列表、字典、集合）：直接修改原对象，无需创建新对象，**不可变对象**（如整数、字符串、元组）`INPLACE_ADD`​实际会创建新对象并重新赋值给变量，总之`+=`​和`-=`​不确定是否原地修改，使用`[:]`​确保原地修改
- [:]：

```python
weight.data[:] -= something 	# 使用[:]完成原地操作，实践中不推荐这么写

# 推荐写法
with torch.no_grad():
    weight -= something
```

| `abs()`    | `abs_()`    |
| ---------- | ----------- |
| `acos()`   | `acos_()`   |
| `add()`    | `add_()`    |
| `addbmm()` | `addbmm_()` |
| ...        |             |





`.norm()`​：计算张量的范数，支持多种范数，默认为𝐿2范数

```python
tensor.norm(p='fro', dim=None, keepdim=False, out=None)
```

**​`p`​**: 范数的类型，常见取值：

- `p=2`​：L2范数，默认值，计算欧几里得范数
- `p=1`​：L1范数，计算元素的绝对值之和
- `p=float('inf')`​：L∞范数，计算最大绝对值元素
- `p='fro'`​：Frobenius 范数（适用于矩阵）

**​`dim`​**: 计算沿着某一维度的范数，结果不是标量

**​`keepdim`​**: 是否保持计算后结果的维度

**​`out`​**: 存放结果的张量

`torch.clamp()`​：裁剪，限制张量的数值在指定范围内

```python
torch.clamp(input, min=None, max=None, *, out=None)
# input: 要裁剪的张量
# min: 最小值，低于min则设为min
# max: 最大值，高于max则设为max
# out: 可选，指定输出张量
```

`torch.where(condition, A, B)`​：若condition为True，则取A，否则取B

| 函数                      | 类型    | 功能简介                                | 行为区别                                |
| ------------------------- | ------- | --------------------------------------- | --------------------------------------- |
| `torch.where(cond, A, B)`​ | PyTorch | 条件选择：cond为True选A，否则选B        | 和NumPy一致，**必须**提供 A、B          |
| `np.where(cond, A, B)`​    | NumPy   | 条件选择：cond为True选A，否则选B        | A、B 可选：不传A、B时返回索引           |
| `df.where(cond)`​          | Pandas  | 对DataFrame中cond为False的地方替换为NaN | 和上面两个不同，是“保留True，替换False” |

### `np.where(cond, A, B)`​ 和 `np.where(cond)`​

两种用法：

- 三参数：跟 PyTorch 一样
- 一参数：返回满足条件的 **索引**

```python
import numpy as np
x = np.array([10, 20, 30])
cond = x > 15
np.where(cond)        # (array([1, 2]),)
np.where(cond, 1, 0)  # array([0, 1, 1])
```

### `pandas.DataFrame.where(cond)`​

- 功能是保留 `cond=True`​ 的位置，其他位置设为 `NaN`​（是“**保留 True**”，和 `torch/np.where`​ 是“**选 True**”的思维方式不同）

```python
import pandas as pd
df = pd.DataFrame({'A': [1, 2, 3]})
cond = df['A'] > 1
df.where(cond)
#     A
# 0  NaN
# 1  2.0
# 2  3.0
```

乘法：

- `torch.matmul()`: 通用矩阵乘法，推荐，支持广播，灵活
- `torch.mm()`: 局限于二维矩阵乘法，不支持广播和批量操作
- `torch.mv()`: 局限于矩阵与向量乘法，不支持广播和批量操作
- `torch.mul()`: 元素级乘法，对应位置的元素相乘

## 常见操作

`.repeat(*sizes)`：将张量在特定维度上重复，返回新张量

- sizes整数元组，与张量形状相同，指定每个维度的重复次数，维度不匹配时
- 不是元素复制，返回的新张量用索引方式复制（类似视图），不占额外空间
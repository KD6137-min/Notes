# 注意力机制

关键在于如何衡量q，k的相似性

## 加性注意力

```math
score(q, k) = v^Ttanh(W_qq+W_kk)
```

不是直接计算差值，而是把q 和 k线性映射后再通过非线性激活融合

差值本质上是一种线性变换的特例

## 缩放点击注意力

Scaled Dot-Product Attention，假设可以用内积衡量相似性，Transformer 架构核心组件之一，用于计算序列中元素之间的关系和重要性

- 核心思想：通过计算查询向量（Query）、键向量（Key）和值向量（Value）之间的关系，来决定如何从值向量中加权组合出一个输出向量
- 具体步骤：

  - 三个输入矩阵：

    - **Query（Q）** ：表示当前要关注的元素
    - **Key（K）** ：表示所有元素的特征
    - **Value（V）** ：与 Key 相对应的元素内容
  - **计算点积**：对每个 Query 向量与所有 Key 向量进行点积，得到一个表示相关性或注意力分数的矩阵
  - **缩放**：将上述分数除以一个缩放因子，通常是 Key 向量的维度的平方根。这是为了防止在计算 softmax 时，点积结果过大导致梯度消失的问题
  - **Softmax**：对归一化后的分数应用 softmax 函数，得到注意力权重，表示各个 Key 对当前 Query 的重要性
  - **加权求和**：用计算得到的注意力权重对 Value 向量进行加权求和，最终得到输出
- 数学表达

  $$
  Attention(Q, K, V) = softmax( \frac{QK^T}{\sqrt {d_k}}) V
  $$

‍

输入 E（每个词的向量）**通过三个不同的线性变换矩阵**生成查询（Query）、键（Key）、值（Value）矩阵:

$Q = EW^Q, K = EW^K, V = EW^V$

这里的W都是可以**学习的参数矩阵**，作用就是从原始表示 E 中学习出适合注意力机制使用的 Q、K、V 表示

$$
head_i = Attention(EW_iQ, EW_iK, EW_iV)
$$

变成

$$
head_i = Attention(QW_iQ, KW_iK, VW_iV)
$$

Q、K、V 只是中间变量，怎么表示都可以，只要数学等价

- 在 **Encoder 自注意力**中，Q\=K\=V\=E
- 在 **Encoder-Decoder attention**中，Q\=decoder hidden，而 K,V\=encoder output

# Transformer

前馈网络功能

在标准Transformer中，每个Encoder/Decoder层包含：

- Multi-Head Attention
- **Feed-Forward Network (FFN)**
- 残差 + LayerNorm

FFN结构：

```math
FFN(x) = max(0, xW_1 + b_1)W_2+b_2
```



通常：

dmodel→W1dff→W2dmodel

典型维度：dmodel=512, dff=2048。

**核心功能：**

1. **非线性映射，提升表达能力**
    - Multi-head attention是线性的，无法捕获复杂特征。
    - FFN加ReLU是通用近似器，可学习更复杂的特征变换。
2. **改变维度，提升特征交互能力**
    - 通常会先升维（512 → 2048），在高维空间做非线性变换，再降回去。
3. **每个位置独立变换，不引入跨位置交互**
    - Attention处理“位置间关系”，FFN处理“单位置特征”。
    - 这样可以分解建模任务：**依赖关系**由Attention建模，**位置内特征提取**由FFN完成。



这里的 **Position-wise** 并不是指“位置有关”，而是指：

- **对每个位置的表示向量独立应用相同的FFN**。

- 假设输入序列是 [x1,x2,…,xn]，每个 xi∈Rdmodel，那么：

    yi=FFN(xi)

    对所有 i，共享同一组参数 W1,W2，但运算彼此独立。

- 这样既能保证处理序列时的并行性，又不破坏序列的顺序敏感性（因为Attention处理顺序信息）。

换句话说：**“Position-wise”强调它不跨位置，而不是它依赖位置**

### **总结一句话**

- **Attention** = 跨位置交互（建模序列依赖）
- **FFN** = 每个位置独立非线性变换（建模局部特征）
- “Position-wise”表示 **逐位置独立应用**，不是指和位置编码绑定



# 缩放

在原始论文（Attention Is All You Need）中，有两个地方涉及到缩放：

- **缩放点积注意力（Scaled Dot-Product Attention）**：

    Attention(Q,K,V)=softmax(QK⊤dk)V

    这是为了防止点积过大导致softmax梯度消失。

- **嵌入缩放（Embedding scaling）**：
    在输入embedding时，论文提到：

    Input=dmodel⋅embedding(x)+positional encoding(x)

    这个乘 dmodel 是为了 **匹配位置编码的量级**。

------

## **2. 为什么不是缩小，而是放大？**

你的直觉是：**embedding的方差应该是 σ2，所以E[∣embedding∣]≈σ，位置编码在[-1,1]，为什么还要乘d让embedding更大？**

原因在于：

- 通常embedding初始化时，每个维度的值是**均值0、方差较小的随机数**，例如：

    embeddingi∼N(0,σ2),  σ≈1d

- 这样每个维度期望大小大约是 1d，整个向量的L2范数期望约为1。

- 如果不做缩放，embedding整体幅度会接近1，而位置编码幅度也在1左右，那么两者量级差不多，但论文认为**embedding应该主导，而不是位置编码掩盖embedding信息**。

- 乘上d后，embedding的幅度整体变为大约 d⋅1d=1，也就是和初始化时的意图一致，同时保证embedding在高维空间仍然有足够的区分度。

所以，这个乘 d 不是放大到比原本的正常embedding大，而是**抵消初始化时按 1d 缩小的影响**，让embedding和位置编码保持合理比例。

------

## **3. 为什么不让位置编码更小？**

可以这么做，但论文采用的设计逻辑是：

- **embedding包含词义，是主信息** → 不能太小，否则训练会慢。
- **位置编码只是辅助** → 即使不缩放，也只在[-1,1]，不会主导输入。

------

### **公式推导一下初始化的方差和缩放结果**

假设embedding维度是 d，每个分量 ∼N(0,1d)，则：

Var(embeddingi)=1d

那么embedding的L2范数期望：

E[∥embedding∥]≈d⋅1d=1

乘 d 后：

∥d⋅embedding∥≈d⋅1=d

而位置编码每个维度 ∈[−1,1]，所以其范数也大约是 d，两者量级匹配。

✅ 也就是说，这个缩放保证了embedding和位置编码的L2范数大致相同，不会出现embedding太小或位置编码掩盖embedding的问题。

------

### **一句话总结**

- 乘 d 并不是无脑放大，而是**为了抵消初始化方差缩放，让embedding和位置编码的量级匹配**。
- 如果不乘，embedding会太小，位置编码会影响过大。

------

要不要我帮你**画一张图，比较 embedding 缩放前后与位置编码的相对大小**，并写个 **PyTorch代码实测embedding初始化后方差和加d的效果**？



RNN 通过循环隐状态传递上下文；Attention 通过计算相关性矩阵显式建模上下文。

## **Transformer 有没有“循环”？**

- **Transformer完全没有时间维度上的递归**：

    - 输入序列是 **并行处理** 的。

    - 每一层对整个序列做 self-attention：

        Attention(Q,K,V)=softmax(QK⊤dk)VAttention(Q,K,V)=softmax(dkQK⊤)V

        其中 Q、K、V 都来自同一序列，但没有顺序依赖。

- 位置顺序信息通过 **位置编码（Positional Encoding）** 注入，而不是循环。

- 所以 Transformer **去掉了RNN的循环结构**，改用 **attention + 前馈网络** 来捕捉依赖关系。

> 换句话说：
>
> - RNN：**递归传递状态**，隐式建模长程依赖，但梯度传播困难。
> - Transformer：**显式建模任意两位置的依赖**，没有递归。





这个问题本质上涉及**感受野、依赖关系建模能力、特征交互方式**。为什么注意力能超越卷积，要从以下几个维度来看：

------

## **1. 卷积的局限性**

卷积（CNN）有两个核心特点：

- **局部感受野**：每个卷积核只看到局部区域，需要多层堆叠才能覆盖全局。
- **参数共享**：空间平移不变性，但缺乏全局依赖建模。
- **长程依赖问题**：要让一个像素与远处像素交互，必须经过多层卷积 → 路径长，信息衰减。

换句话说，CNN对全局依赖建模效率低，尤其是序列或长文本、长程空间关系。

------

## **2. 注意力的优势**

注意力（尤其是 **自注意力 Self-Attention**）有两个核心优势：

1. **全局感受野**：
    每个位置与序列中所有位置直接交互，复杂度 O(n2)，但一次就能看到全局。

    Attention(Q,K,V)=softmax(QK⊤dk)V

    这里 QK⊤ 直接计算所有位置两两相似度，信息流动更快。

2. **动态建模依赖**：
    卷积的核是固定权重，不能针对输入内容动态调整。
    注意力权重 αij 由输入数据计算，可针对当前任务自适应选择信息来源。

------

## **3. 注意力是如何“挖掘深层特征”的？**

**卷积挖掘深层特征的方式**：

- 堆叠多层，逐步增加感受野，从局部纹理 → 边缘 → 物体 → 场景。

**注意力挖掘深层特征的方式**：

- 每一层的自注意力都能建立全局依赖，但不是直接用，而是通过 **多头机制 (Multi-Head Attention)** 和 **层叠**提取不同粒度特征：
    - **低层注意力头**：倾向于局部模式（类似卷积）。
    - **高层注意力头**：倾向于全局依赖，捕获语义概念。
- 叠加 FFN（非线性映射）后，每层输出能组合不同上下文，形成更抽象的特征。

**关键区别**：
卷积的特征抽象依赖于空间邻域，注意力的特征抽象依赖于**全局信息重加权**，能快速整合上下文。

------

## **4. 为什么 Transformer 能替代 CNN？**

在视觉领域，**ViT (Vision Transformer)** 证明注意力也能处理图像，因为：

- 图像patch序列输入Transformer，注意力可以捕获跨patch的全局关系，比卷积逐步扩展感受野更快。
- 深层注意力层叠后，逐渐从低级纹理到高级语义。
- 大量数据 + 正则化手段，使其泛化性能强。

CNN仍有优势（局部归纳偏置，计算高效），但当有大数据和算力时，Transformer更灵活。

------

### ✅ **一句话总结**

- **卷积**：固定局部模式 → 堆叠实现全局。
- **注意力**：动态全局建模 → 每一层都能访问全局特征。
- 深层注意力网络通过多头、层叠、非线性组合实现从低级到高级特征的抽象，甚至比卷积更快、更灵活。

------

要不要我帮你：

- **画一张对比图**（CNN vs Attention，展示感受野和特征流动路径）？
- **给一个PyTorch小实验**，展示卷积 vs 注意力在长程依赖任务上的性能差异（例如序列复制任务）？

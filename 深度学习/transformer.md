、、、









































## BERT

Bidirectional Encoder Representations from Transformers，基于Transformer

|                |                                                              |
| -------------- | ------------------------------------------------------------ |
| **核心创新**   | 深层**双向**上下文理解（通过MLM任务实现）                    |
| **模型基础**   | Transformer架构的**编码器（Encoder）** 部分                  |
| **训练方式**   | 两阶段流程：**预训练（Pre-training）** → **微调（Fine-tuning）** |
| **预训练任务** | **遮蔽语言模型（Masked Language Model, MLM）** 和 **下一句预测（Next Sentence Prediction, NSP）** |
| **应用场景**   | 文本分类、问答系统、命名实体识别、情感分析等                 |



1. **双向编码器**：在BERT之前，像GPT这样的模型是单向的，只能从左到右或从右到左理解文本。BERT的核心创新在于其**深层双向架构**。它通过**遮蔽语言模型（MLM）** 任务，在训练中随机遮盖输入序列中的部分词（如15%），然后要求模型根据这个词的**左右两侧上下文**来预测被遮盖的词是什么。这使BERT能更深刻地理解每个词在具体语境中的真实含义。
2. **Transformer编码器**：BERT完全基于Transformer模型中的**编码器部分**构建。编码器中的**自注意力机制（Self-Attention）** 能让序列中的每个词同时与序列中的所有其他词进行交互，从而有效捕捉长距离依赖关系，并且便于并行计算，大大提升了训练效率。
3. **预训练任务**： **MLM任务**：如上所述，是BERT理解词义的关键。 **下一句预测（NSP）**：为了帮助模型理解句子间的关系（如问答、自然语言推理），BERT还进行了NSP任务训练。即输入两个句子A和B，模型需要判断句子B是否是句子A在原文中的下一句。



**推动迁移学习**：确立了“**预训练+微调**”的范式作为现代NLP研究与应用的基础流程




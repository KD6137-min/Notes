图片缩小：`magic`命令

```bash
magic input.jpg -resize 200x200 -quality 50 output.jpg
```

- `-resize nxn`或`-resize n%`



arch命令

```bash
uname -r	# 查看内核、用户
```



snapper

典型Btrfs子卷布局：

```plaintext
/                     # 根文件系统（挂载自 Btrfs 卷）
├── @root             # 系统根目录的子卷
├── @home             # 用户数据的子卷
├── @snapshots        # 存储所有快照
│   ├── @root-1       # 快照 1（只读）
│   ├── @root-2       # 快照 2
│   └── ...
└── .snapshots        # Snapper 的快照元数据（符号链接到 @snapshots）
```

常用命令：

```bash
# 为根目录创建默认配置
sudo snapper create-config /
# 查看现有快照
sudo snapper list

# snapper自动快照会按计划创建快照，策略定义在/etc/snapper/configs/root
# 手动快照
sudo snapper create --description "Before ..."

# 回滚到指定快照(按快照编号)
sudo snapper undochange <number>..0 	# 仅撤销文件更改
sudo snapper rollback <number>		# 完整回滚，需重启生效

# 从快照恢复单个文件
ls /.snapshots/<number>/snapshot/home/user/important_file.txt
cp /.snapshots/<number>/snapshot/home/user/important_file.txt ~/

# 清理快照
sudo snapper delete <number>	# 删除单个快照
sudo snapper cleanup timeline	# 按策略自动清理
```





conda命令

```bash
which conda
conda --version
conda env list

# 删除旧环境
conda remove --name old_env --all
```

jupyter命令

```bash
jupyter kernelspec list 	# 查看内核
jupyter kernelspec remove old_kernel 	# 删除旧内核
```



nvim使用





深度学习路线

1. 异常处理，numpy、matplotlib, pandas,

2. minist手写字体识别、花朵分类、resnet跑分类之类的任务代码，过一遍，然后debug

3. yolo系列：
    ◦ v1-V3，看博客，算法原理，提升trick

    ◦ v4，论文和博客，了解理论，带着理论去debug v5的源码

    ◦ v7和v8

4. ssd、两阶段的faster rcnn、cascade rcnn

5. transformer，vit、detr、deformable detr 

6. unet系列（unet 、unet++、unet3+ ）

7. deeplab系列（v1到v3+）

8. SAM

9. maskformer系列。实例分割的mask rcnn

10. 经典下游任务：目标跟踪姿态估计 行为识别等等，deepsort openpose





### 一、基础工具阶段

1. **异常处理与数据科学三件套**
    - **核心目标**：通过真实场景掌握`NumPy`（张量操作）、`Pandas`（数据清洗）、`Matplotlib`（可视化）的高效组合使用，例如在数据增强或特征分析时处理缺失值、异常样本
    - **关键实践**：在分类任务（如花朵分类）中，**主动构造异常数据**（如损坏图像、错误标签），编写健壮的数据加载器（`Dataset`类），避免后续模型训练因数据问题崩溃
2. **分类任务代码精修**
    - **Minist手写识别**：不满足于跑通代码，需通过Hook机制可视化中间特征图，理解卷积层如何提取边缘/纹理
    - **ResNet实战**：重点调试残差连接，对比有无残差时梯度反向传播的差异（可用`torch.autograd.grad`检查梯度值，可视化中间特征图），理解其解决梯度消失的原理

### **二、核心模型学习阶段分步策略**

#### **目标检测：从理论到源码级掌握**

1. **YOLO系列进阶路线**
    - **v1-v3理论**：结合论文精读+源码分析，重点理解**网格预测机制**、**多尺度融合**（FPN）及**损失函数设计**（如v3的二元交叉熵替代Softmax）
    - v4/v5源码调试：带着问题看代码：
        - 例如v4的**Mish激活函数**对梯度传播的影响（对比ReLU训练曲线）
        - v5的**自适应锚框计算**（`k-means`聚类实现）
    - **v7/v8新特性**：关注v8的**无锚点设计**（Task-Aligned Assigner）和**分布式训练支持**（DDP优化）
2. **两阶段检测器对比学习**
    - **Faster R-CNN**：深入调试RPN模块，理解候选框生成与RoIAlign的坐标映射关系
    - **Cascade R-CNN**：重点分析级联检测头的训练策略（阶段间IoU阈值递增），可视化各阶段预测框质量提升

#### **Transformer在CV的演化实践**

1. **ViT**：调试`patch embedding`层，理解如何将图像转为序列，并可视化位置编码的效果（如旋转不变性测试）
2. DETR与Deformable DETR：
    - 对比训练效率：DETR收敛慢问题（需500epoch+），Deformable的可变形注意力如何加速
    - 关键代码：`MultiScaleDeformableAttention`模块的采样点偏移预测机制

#### **分割模型技术栈深化**

1. **UNet家族**
    - **基础UNet**：调试跳跃连接中特征融合方式（通道拼接 vs 相加）
    - **UNet++**：分析嵌套跳跃结构对梯度流动的影响（参数量激增问题）
    - **UNet3+**：重点实验全尺度跳跃连接在医疗图像小目标分割中的效果
2. **DeepLab系列演进**
    - **v1-v2**：调试空洞卷积（Dilated Conv）在多尺度特征提取中的作用（对比普通卷积感受野）
    - **v3+**：结合Encoder-Decoder与ASPP模块，可视化不同膨胀率的特征响应
3. **前沿分割模型**
    - **SAM**：通过Prompt工程调试（点/框提示），理解零样本泛化能力边界
    - **MaskFormer**：分析基于Query的实例分割如何统一语义/实例分割任务

### **三、下游任务衔接关键点**

1. **目标跟踪（DeepSORT）**：在YOLO检测基础上，调试运动预测（卡尔曼滤波）与ReID特征关联的协同机制
2. 姿态估计（OpenPose）：
    - 重点解析**多分支热图预测**（Part Affinity Fields）
    - 实践**多人姿态组装算法**的代码实现
3. **行为识别**：结合时空模型（如3D CNN、TSN），调试视频帧采样策略对长时序建模的影响

### **四、学习策略强化与资源推荐**

1. **Debug方法论**：
    - 使用`torchviz`可视化计算图，定位梯度异常
    - 利用`hook`机制拦截中间层输出（如检测头分类置信度分布）
    - 对损失函数进行数值稳定性测试（如极端值输入）
2. **必备工具链**：
    - **框架**：PyTorch（研究首选）+ MMDetection（高效复现SOTA）
    - **部署**：ONNX/TensorRT模型转换（YOLOv8需调试导出精度）
    - **实验管理**：Weights & Biases（超参数追踪）
3. **高效学习资源**：
    - **课程**：李沐《动手学深度学习》（PyTorch版）
    - **代码库**：OpenMMLab（MMDet/MMSeg集成主流算法）
    - **论文精读**：从目标检测综述（如《Object Detection in 20 Years》）切入建立体系

> 注意：
>  🔹 ​**​避免陷入理论空转​**​——每个模型学习需配套：源码调试（30%）+ 参数实验（50%）+ 论文复现（20%）
>  🔹 ​**​警惕技术碎片化​**​——以YOLO为主线贯穿检测/分割/跟踪任务，形成技术闭环
>  🔹 ​**​优先掌握行业级工具​**​：ONNX部署、MMLab生态、分布式训练，直接影响工程落地效率



# **Hook机制**

劫持程序控制流，**在程序执行流程中插入自定义代码**，用于拦截和修改系统事件、函数调用或消息传递的行为

本质是**“中间件”**

核心思想是在特定执行点（称为“钩子点”）注册回调函数，当程序运行到该点时自动触发自定义逻辑，实现对原行为的监控、增强（在函数执行前后插入额外逻辑）或重定向

框架级 Hook：PyTorch中捕获模型中间结果（特征图、梯度）

- **前向钩子**（`register_forward_hook`）：捕获层输入/输出（如可视化 ResNet 特征图）
- **反向钩子**（`register_full_backward_hook`）：监控或修改梯度（如实现 Grad-CAM 热力图）

在 PyTorch 中，Hook 是**理解模型内部状态的核心工具**：

1. **特征可视化**：通过前向 Hook 提取卷积层输出，生成 Grad-CAM 热力图解释分类决策
2. **梯度分析**：反向 Hook 监控梯度消失/爆炸，辅助调试优化器问题
3. 动态修改训练
    - 冻结指定层梯度（修改反向传播结果）
    - 实现自定义正则化（如梯度裁剪）



# **ONNX部署**

**核心价值**：模型跨平台落地的核心，解决PyTorch模型在工业场景（如嵌入式设备、移动端）的跨框架、跨硬件部署问题，实现训练与推理环境解耦
**工作流程**：

1. 模型导出：使用 `torch.onnx.export`将PyTorch模型转为ONNX格式，注意关键参数：

    ```python
    torch.onnx.export(
        model, 
        dummy_input,  # 示例输入（需包含动态维度，如batch_size）
        "model.onnx", 
        opset_version=12,  # 算子集版本（需与推理环境兼容）
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}}  # 支持动态batch
    )
    ```

2. 模型验证：

    - 使用 `onnx.checker.check_model()` 检查格式正确性
    - 通过 Netron 可视化计算图（如卷积层→Gemm层的转换）

3. 推理加速：

    ```python
    import onnxruntime as ort
    sess = ort.InferenceSession("model.onnx", providers=["CUDAExecutionProvider"])  # GPU加速
    outputs = sess.run(None, {"input": input_array})[0]  # 输入需为NumPy数组（形状：BCHW）
    ```

**避坑指南**：

- 动态输入需显式声明 `dynamic_axes`，否则固定batch影响部署灵活性
- ONNX Runtime与CUDA版本需严格匹配（如CUDA 11.x对应onnxruntime-gpu==1.15）



# **OpenMMLab**

**核心价值**：视觉算法的工业化实践框架，提供模块化、标准化的视觉算法实现（目标检测/分割/姿态估计等），大幅降低复现SOTA模型的成本
 **关键组件**：

- **MMCV**：基础库（Hook机制、分布式训练接口）
- **MMDetection**：支持YOLO系列、Faster R-CNN等检测模型
- **MMSegmentation**：集成UNet、DeepLab等分割模型
- **MMPose**：实现OpenPose等姿态估计算法

**​实战流程​**​：

1. 安装与配置：

    ```python
    pip install mmcv-full mmdet mmsegmentation  # 安装核心组件
    ```

2. 快速调用模型（以YOLOv3为例）：

    ```python
    from mmdet.apis import init_detector, inference_detector
    
    config = "configs/yolo/yolov3_d53_320_273e_coco.py"
    checkpoint = "https://download.openmmlab.com/mmdetection/v2.0/yolo/yolov3_d53_320_273e_coco/yolov3_d53_320_273e_coco-421362b5.pth"
    
    model = init_detector(config, checkpoint, device="cuda:0")
    result = inference_detector(model, "image.jpg")  # 一键推理
    ```

3. **Hook扩展**：通过注册Hook拦截训练中间结果（如特征图、梯度），用于可视化或自定义逻辑



# 分布式训练

**核心价值**：加速大规模视觉模型训练，利用多GPU/多机并行，解决视觉模型（如高分辨率图像分割）训练速度瓶颈
 **PyTorch实现方案**：

1. **数据并行（Data Parallel, DP）**：单机多卡（已淘汰）

2. 分布式数据并行（DDP）：多机多卡（推荐）

    ```python
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    
    # 初始化进程组
    dist.init_process_group(backend="nccl", init_method="env://")
    model = DDP(model, device_ids=[local_rank])  # 包裹模型
    ```

**关键技术点**：

- **梯度同步**：各GPU计算梯度后求平均，确保参数一致性
- **混合精度训练**：结合FP16+FP32，提升速度并减少显存占用

 ​**​实践建议​**​：

- 使用 `torchrun` 启动训练（自动处理进程调度）
- 小批量数据需搭配梯度累积（`accumulate_grad_batches`）避免精度下降



# 消融实验（Ablation Study）

**核心价值**：模型创新的科学验证，通过控制变量法，量化评估模型中各组件（模块/超参/数据策略）的贡献，避免“玄学调参”
 **实验设计原则**：

1. **单变量控制**：每次仅修改一个组件（如移除注意力层、替换损失函数）
2. **对比指标**：选择任务相关指标（目标检测→mAP@0.5/0.75；分割→IoU）
3. **充分训练**：每个实验需独立训练至收敛，固定随机种子确保可复现性

**实例解析**（目标检测任务）：

| 实验组             | mAP@0.5 | mAP@0.75 | 小目标精度 | 结论              |
| ------------------ | ------- | -------- | ---------- | ----------------- |
| 完整模型 (A+B+C)   | 0.82    | 0.74     | 0.68       | 基准性能          |
| 移除HSR模块 (A+C)  | 0.78↓   | 0.70↓    | 0.60↓      | HSR提升小目标检测 |
| 移除SFDL模块 (A+B) | 0.80↓   | 0.72↓    | 0.65↓      | SFDL优化定位精度  |
| 仅基础模型 (A)     | 0.75↓   | 0.65↓    | 0.55↓      | 所有模块均有贡献  |

消融实验是论文核心证据，缺少则创新点可信度不足



